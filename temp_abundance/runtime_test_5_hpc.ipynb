{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d308d",
   "metadata": {
    "id": "121d308d"
   },
   "outputs": [],
   "source": [
    "# from github temp.jl, converted to python\n",
    "\n",
    "# this section is about incorporating temperature dependence\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "rng = default_rng(111) # random number generator\n",
    "\n",
    "\n",
    "# note that the u(T) and m(T) equation have the same structure, except the parameters are different.\n",
    "# both of the equations involve parameters like:\n",
    "# B0 (base rate), E (activation energy), k (Boltzmann constant), Ed (deactivation energy), Tp (peak performance temperature), and Tr (reference temperature)\n",
    "# first, we want to generate random parameters for each consumer (as mentioned above), like B, E, Tp\n",
    "# and for each of these temp-dependent parameters, we want to generate one for uptake u(T) and one for respiration m(T)\n",
    "# they're not actually fully random, as we do define mean and variance for teh parameters, and draw from a multivariate normal distribution\n",
    "\n",
    "\n",
    "\n",
    "def randtemp_param(N, kw):\n",
    "    \"\"\"\n",
    "    Generate random temperature-dependent trait parameters for consumers.\n",
    "\n",
    "    Returns:\n",
    "        B: base rates (N x 2)\n",
    "        E: activation energies (N x 2)\n",
    "        Tp: peak temperatures (N x 2)\n",
    "    \"\"\"\n",
    "    L = kw['L'] # leakage\n",
    "    rho_t = kw['rho_t']\n",
    "\n",
    "    L_v = np.mean(L)\n",
    "    B0_m = -1.4954 # I think this is mortality / respiration rate\n",
    "    B0_CUE = 0.1953\n",
    "    B0_u = np.log(np.exp(B0_m) / (1 - L_v - B0_CUE)) # I think this is uptake rate. dependent on carbon use efficiency (CUE) and leakage rate\n",
    "\n",
    "    B0 = np.array([B0_u, B0_m]) # B0 is a vector of base rates for uptake and respiration\n",
    "    B0_var = 0.17 * np.abs(B0) # variance of base rates, 0.17 is a scaling factor\n",
    "    E_mean = np.array([0.8146, 0.5741]) # mean activation energies for uptake and respiration\n",
    "    E_var = 0.1364 * E_mean # variance of activation energies, 0.1364 is a scaling factor\n",
    "    cov_xy = rho_t * np.sqrt(B0_var * E_var) # covariance between base rates and activation energies, rho_t is the correlation coefficient\n",
    "\n",
    "    cov_u = np.array([[B0_var[0], cov_xy[0]], [cov_xy[0], E_var[0]]]) # covariance matrix for uptake\n",
    "    cov_m = np.array([[B0_var[1], cov_xy[1]], [cov_xy[1], E_var[1]]]) # covariance matrix for respiration\n",
    "\n",
    "    allu = multivariate_normal.rvs(mean=[B0[0], E_mean[0]], cov=cov_u, size=N).T # draw random samples from multivariate normal distribution for uptake\n",
    "    allm = multivariate_normal.rvs(mean=[B0[1], E_mean[1]], cov=cov_m, size=N).T # draw random samples from multivariate normal distribution for respiration\n",
    "\n",
    "    B = np.column_stack((np.exp(allu[0]), np.exp(allm[0]))) # exponentiate the base rates to get the actual values\n",
    "    E = np.column_stack((allu[1], allm[1])) # activation energies are already in the correct form\n",
    "\n",
    "    Tpu = 273.15 + rng.normal(35, 5, N) # draw random peak temperatures for uptake from a normal distribution with mean 35 and std 5\n",
    "    Tpm = Tpu + 3 # peak temperature for respiration is 3 degrees higher than for uptake\n",
    "    Tp = np.column_stack((Tpu, Tpm)) # combine the peak temperatures into a single array\n",
    "\n",
    "    return B, E, Tp\n",
    "\n",
    "\n",
    "# randtemp_param_test = randtemp_param(3, {'L': 0.4, 'rho_t': -0.75})\n",
    "# print(randtemp_param_test)\n",
    "# this works - produces 2D arrays\n",
    "\n",
    "\n",
    "# now we have generated parameters (generating B, E, Tp) for each consumer\n",
    "# we did this by drawing from a multivariate normal distribution, with some constraints like mean, variance, correlation that we defined\n",
    "# now that the parameters are generated, we can incorporate them into the Arrhenius equation to calculate the temperature-dependent trait values\n",
    "# since uptake u(T) and respiration m(T) both depend on these parameters like B, E, Tp (which we have now defined)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def temp_trait(N, kw):\n",
    "    \"\"\"\n",
    "    Compute temperature-dependent trait scaling based on an Arrhenius-like thermal performance curve.\n",
    "\n",
    "    Arguments:\n",
    "        N: number of consumers\n",
    "        kw: dict containing temperature parameters (T, Tr, Ed, L, rho_t)\n",
    "\n",
    "    Returns:\n",
    "        temp_p: temperature-scaled trait values (vector of size N)\n",
    "        B, E: base rates and activation energies (N x 2)\n",
    "        Tp: peak temperatures for uptake and respiration (N x 2)\n",
    "    \"\"\"\n",
    "    k = 0.0000862  # Boltzmann constant. used in exponential term of the Arrhenius equation\n",
    "    T = kw['T']\n",
    "    Tr = kw['Tr']\n",
    "    Ed = kw['Ed']\n",
    "\n",
    "    B, E, Tp = randtemp_param(N, kw) # draw random base rates (B), activation energies (E), and peak temperatures (Tp) for each consumer.\n",
    "    # we are using the previously defined function to generate these parameters\n",
    "\n",
    "    # Arrhenius function with high-temp deactivation\n",
    "\n",
    "    # uptake rate u(T)\n",
    "    temp_p_u = B[:, 0] * np.exp((-E[:, 0] / k) * ((1 / T) - (1 / Tr))) / \\\n",
    "              (1 + (E[:, 0] / (Ed - E[:, 0])) * np.exp(Ed / k * (1 / Tp[:, 0] - 1 / T)))\n",
    "\n",
    "    # respiration rate m(T)\n",
    "    temp_p_m = B[:, 1] * np.exp((-E[:, 1] / k) * ((1 / T) - (1 / Tr))) / \\\n",
    "              (1 + (E[:, 1] / (Ed - E[:, 1])) * np.exp(Ed / k * (1 / Tp[:, 1] - 1 / T)))\n",
    "\n",
    "    temp_p = np.column_stack((temp_p_u, temp_p_m))  # shape (N,2)\n",
    "\n",
    "    return temp_p, B, E, Tp\n",
    "\n",
    "\n",
    "# temp_trait_test = temp_trait(3, {'T': 273.15 + 10, 'Tr': 273.15 + 10, 'Ed': 3.5, 'L': 0.4, 'rho_t': -0.75})\n",
    "# print(temp_trait_test)\n",
    "\n",
    "\n",
    "# this temp_trait function uses the previous randtemp_param function to generate parameters for TPC, which are B, E, Tp\n",
    "# and then it uses these parameters to calculate the temperature-dependent trait values (u and m, which are uptake and respiration)\n",
    "# it does this by putting these parameters into the Arrhenius-like TPC equations u(T) and m(T)\n",
    "# the inputs are N (number of consumers) and kw (a dictionary of parameters like T, Tr, Ed, L, rho_t)\n",
    "# and the outputs are the temp_p (temperature-scaled trait values), B (base rates), E (activation energies), and Tp (peak temperatures)\n",
    "# we could define some temperature T, and the temp_p functions would calculate the resulting u and m for that temperature\n",
    "# so using this, we can compare e.g. low vs high temperatures by defining different T values\n",
    "# and potentially make continuous graphs of temperature vs growth rate or something, and use that to inform timescale separation / GLV accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42b142a",
   "metadata": {
    "id": "d42b142a"
   },
   "outputs": [],
   "source": [
    "# from github micrm_params.jl, converted to python\n",
    "\n",
    "# in the previous block of code, we generated temperature-dependent parameters\n",
    "# but those parameters were just for the TPC\n",
    "# we also need to produce the other parameters for MiCRM\n",
    "# so this block of code focuses on defining parameters for MiCRM\n",
    "# and we later use these parameters to run simulations etc\n",
    "\n",
    "\n",
    "# kw is a dictionary carrying all the model inputs and intermediate results (temp, TPC scaling, leakage scalar, etc)\n",
    "# we like to include it (like in def_m) even though it's not needed for the default version\n",
    "# so that any generator function can just pull out what it needs from kw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Default parameter-generating functions\n",
    "def def_m(N, M, kw):\n",
    "    # Respiration (mortality) rates: ones vector of length N.\n",
    "    return np.ones(N)\n",
    "\n",
    "def def_rho(N, M, kw):\n",
    "    # Resource-specific supply rates: ones vector of length M.\n",
    "    # this was originally resource loss rate, but i changed it to supply so it makes sense with the equation\n",
    "    return np.ones(M)\n",
    "\n",
    "def def_omega(N, M, kw):\n",
    "    # Resource loss rates: ones vector of length M.\n",
    "    return np.ones(M)\n",
    "\n",
    "\n",
    "# the above code for def_m and def_rho are just returning vectors of ones, length N for respiration (m) and length M for supply (rho).\n",
    "# these are like the 'default' respiration/mortality rates, and default resource supply rates. def = default\n",
    "# depends on number of consumers N and number of resources M\n",
    "# later we can add temperature dependence, using functions we defined in previous section of code\n",
    "\n",
    "\n",
    "\n",
    "def def_u(N, M, kw):\n",
    "    # Uptake matrix: each of N rows is drawn from a symmetric Dirichlet of dimension M.\n",
    "    # this establishes the structure of the uptake function - it's a matrix of N x M\n",
    "    # unlike others like respiration (m) which is just a vector of length N\n",
    "        \n",
    "    rng = kw.get('rng', np.random)\n",
    "    return rng.dirichlet(np.ones(M), size=N)\n",
    "\n",
    "\n",
    "# the 'u' is an N x M matrix. each of the rows are sampled from a Dirichlet distribution.\n",
    "# each consumer is a row, and each resource is a column.\n",
    "# each consumer's uptake across resources (sum of a row) is 1. but this is randomly allocated among M resources.\n",
    "# so i guess the uptake is not absolute units but it's relative to other resources?\n",
    "# also i think this describes uptake preference (so how much of a certain resource it uptakes vs another one, relatively), not uptake rate which can be temp-dependent (as previously defined)\n",
    "\n",
    "\n",
    "def def_l(N, M, kw):\n",
    "    # Leakage-transformation tensor: shape (N, M, M). For each consumer i and resource alpha,\n",
    "    # draw an M-vector from Dirichlet and scale by L[i].\n",
    "    L = kw['L']\n",
    "    rng = kw.get('rng', np.random) \n",
    "    l = np.zeros((N, M, M))\n",
    "    phi = np.ones(M)\n",
    "    for i in range(N):\n",
    "        for alpha in range(M):\n",
    "            draw = rng.dirichlet(alpha=phi)\n",
    "            l[i, alpha, :] = draw * L[i]\n",
    "    return l\n",
    "\n",
    "# again, i think leakage is relative units (arbitrary units or sth)\n",
    "# more about the 'preference' in terms of which resources are leaked\n",
    "\n",
    "\n",
    "# this is the N x M x M leakage tensor. for each consumer i and resource index alpha, we are drawing an M-vector from a Dirichlet distribution, and scaling it by L[i].\n",
    "# this is the leakage matrix for each consumer-resource pair.\n",
    "# from the paper:\n",
    "# L encodes each strain's metabolic network. L is the leakage-transformation tensor. L determines how consumed substrates are leaked / metabolically transformed\n",
    "# so i think for each strain N + each resource M, there is a whole vector (length M) of what resources that this one coudl be leaked as or transformed into.\n",
    "\n",
    "# these previous functions are 'default' functions for generating parameters\n",
    "# they create simple placeholder values like vectors of 1s, or random Dirichlet distributions\n",
    "# these functions can be overridden (e.g. after we add temp dependence)\n",
    "\n",
    "# next, in generate_params, we will add temperature dependence to the parameters\n",
    "\n",
    "def generate_params(N,\n",
    "                     M,\n",
    "                     f_m=def_m,\n",
    "                     f_rho=def_rho,\n",
    "                     f_omega=def_omega,\n",
    "                     f_u=def_u,\n",
    "                     f_l=def_l,\n",
    "                     **kwargs):\n",
    "\n",
    "    # f_m = def_m means that if we don't provide a function for f_m, it will use the default one (def_m)\n",
    "    # this is why we had to define the default functions earlier\n",
    "    # **kwargs is any other keyword arguments (not directly specified as function inputs here, but bundled together in dictionary)\n",
    "\n",
    "    \"\"\"\n",
    "    Generate temperature-dependent MiCRM parameters.\n",
    "\n",
    "    Parameters:\n",
    "        N, M       : integers, number of consumers and resources\n",
    "        f_m, f_rho, f_omega, f_u, f_l : functions to generate m, rho, omega, u, l\n",
    "        kwargs     : other keyword arguments (e.g., T, rho_t, Tr, Ed, L)\n",
    "\n",
    "    Returns:\n",
    "        params : dict with keys 'N', 'M', 'u', 'm', 'l', 'rho', 'omega', 'lambda',\n",
    "                 plus temperature traits and any extra kwargs\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy kwargs for internal use\n",
    "    kw = dict(kwargs)\n",
    "\n",
    "    # Temperature-dependent traits, as defined in previous code block\n",
    "    tt, B, E, Tp = temp_trait(N, kw) # according to previously defined temp_trait function\n",
    "    # tt is the first output of the temp_trait function, which is temp_p\n",
    "\n",
    "\n",
    "    kw['tt'] = tt\n",
    "\n",
    "    # in this section above:\n",
    "    # we plug in N and kw (kw is a dictionary of parameters), both of which are inputs to the overall generate_params function\n",
    "    # and using a previously defined function temp_trait, we get parameters for temperature dependence\n",
    "    # the parameters generated include temp_p_u and temp_p_m\n",
    "    # now that we've generated temp_p_u and temp_p_m, we want to store them into kw dictionary\n",
    "\n",
    "    # Generate consumer parameters\n",
    "    m = f_m(N, M, kw) # Respiration (mortality) rates\n",
    "    u = f_u(N, M, kw) # Uptake matrix\n",
    "\n",
    "    # this section above:\n",
    "    # f_m and f_u generate the final parameters for uptake and mortality\n",
    "    # we haven't defined a custom function for f_m or f_u, so it will use the default ones we defined earlier\n",
    "    # so here, it's basically same as m = def_m(N, M, kw) and u = def_u(N, M, kw)\n",
    "\n",
    "\n",
    "    # Leakage-transformation tensor\n",
    "    l = f_l(N, M, L)      # Leakage-transformation\n",
    "\n",
    "    # Total leakage per consumer-resource pair\n",
    "    lambda_ = np.sum(l, axis=2)  # shape (N, M)\n",
    "\n",
    "    # Resource parameters\n",
    "    rho = f_rho(N, M, kw)\n",
    "    omega = f_omega(N, M, kw)\n",
    "\n",
    "    # Assemble base parameter dict\n",
    "    params = {\n",
    "        'N': N,\n",
    "        'M': M,\n",
    "        'u': u,\n",
    "        'm': m,\n",
    "        'l': l,\n",
    "        'rho': rho,\n",
    "        'omega': omega,\n",
    "        'lambda': lambda_,\n",
    "        'L': L,\n",
    "        'B': B,\n",
    "        'E': E,\n",
    "        'Tp': Tp,\n",
    "        'tt': tt\n",
    "    }\n",
    "    # Merge in any extra user-supplied kwargs\n",
    "    params.update(kwargs)\n",
    "\n",
    "    return params # so it shows the base parameter dictionary we defined within this function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test generate_params just to show how it works:\n",
    "# given N, M, and other parameters like temperature, it generates parameters for MiCRM\n",
    "# and there is an element of stochasticity, because the community assemblies are random\n",
    "\n",
    "# params = generate_params(\n",
    "#     N=10,\n",
    "#     M=5,\n",
    "#     T=310,\n",
    "#     Tr=275,\n",
    "#     Ed=2,\n",
    "#     L=np.random.uniform(0.1, 0.5, size=N),\n",
    "#     rho_t=1\n",
    "# )\n",
    "\n",
    "# print (params)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2cba08e",
   "metadata": {
    "id": "b2cba08e"
   },
   "outputs": [],
   "source": [
    "# from github dx_v2.jl, converted to python\n",
    "\n",
    "\n",
    "# now that we've generated parameters, we can write the actual MiCRM model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def MiCRM_dxx(x, t, N, M, u, l, rho, omega, m):\n",
    "    dx = np.zeros(N + M)\n",
    "    # consumer dynamics\n",
    "    for i in range(N):\n",
    "        dx[i] = -m[i] * x[i]\n",
    "        for alpha in range(M):\n",
    "            res_idx = N + alpha\n",
    "            uptake = x[i] * x[res_idx] * u[i, alpha]\n",
    "            dx[i] += uptake\n",
    "            for beta in range(M):\n",
    "                dx[i] -= uptake * l[i, alpha, beta]\n",
    "    # resource dynamics\n",
    "    for alpha in range(M):\n",
    "        idx = N + alpha\n",
    "        dx[idx] = rho[alpha] - omega[alpha] * x[idx]\n",
    "        for i in range(N):\n",
    "            dx[idx] -= u[i, alpha] * x[idx] * x[i]\n",
    "            for beta in range(M):\n",
    "                dx[idx] += x[N + beta] * x[i] * u[i, beta] * l[i, beta, alpha]\n",
    "    return dx\n",
    "\n",
    "# In solve_ivp, use a wrapper to pass jit arguments:\n",
    "def MiCRM_dxx_numba_wrapper(t, x, p):\n",
    "    # Unpack parameters directly for the JIT function\n",
    "    return MiCRM_dxx(x, t,\n",
    "                         p['N'], p['M'],\n",
    "                         p['u'], p['l'],\n",
    "                         p['rho'], p['omega'],\n",
    "                         p['m'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8b502",
   "metadata": {
    "id": "d4e8b502"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eff_LV_params(p, sol, verbose=False):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of effective Lotka–Volterra parameters.\n",
    "\n",
    "    Parameters:\n",
    "        p       : dict with MiCRM parameters ('N','M','u','l','rho','omega','m','lambda', etc.)\n",
    "        sol     : ODE solution object for MiCRM (with .y containing shape (N+M, T))\n",
    "        verbose : if True, also return intermediate matrices 'dR' and 'A'\n",
    "\n",
    "    Returns:\n",
    "        result : dict containing\n",
    "          - 'alpha' : (N×N) effective interaction matrix\n",
    "          - 'r'     : (N,) intrinsic LV growth rates\n",
    "          - 'N'     : int, number of species\n",
    "        plus, if verbose=True:\n",
    "          - 'dR'    : (M×N) resource sensitivity matrix\n",
    "          - 'A'     : (M×M) resource Jacobian\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    N = p['N']\n",
    "    M = p['M']\n",
    "    u   = p['u']        # (N, M)\n",
    "    lam = p['lambda']   # (N, M)\n",
    "    rho = p['rho']      # (M,)\n",
    "    omega = p['omega']  # (M,)\n",
    "    m   = p['m']        # (N,)\n",
    "    \n",
    "    # Equilibrium values\n",
    "    Ceq = sol.y[:N, -1]  # (N,)\n",
    "    Req = sol.y[N:, -1]  # (M,)\n",
    "\n",
    "    # 1) Build A (M×M):\n",
    "    #    A[α,β] = -ω[α] + sum_i l[i,α,β]*u[i,β]*Ceq[i]\n",
    "    #              - δ(α,β)*sum_i u[i,β]*Ceq[i]\n",
    "    A = -np.diag(omega)\n",
    "    W = u * Ceq[:, None]  # (N, M)\n",
    "    # add sum_i l[i,α,β] * W[i,β]\n",
    "    A += np.einsum('ib,iab->ab', W, p['l'])\n",
    "    # subtract sum_i u[i,α]*Ceq[i] on diagonal\n",
    "    diag_sub = W.sum(axis=0)  # (M,)\n",
    "    A[np.diag_indices(M)] -= diag_sub\n",
    "\n",
    "    # 2) Invert A\n",
    "    invA = np.linalg.inv(A)\n",
    "\n",
    "    # 3) Compute dR = ∂R/∂C (M×N)\n",
    "    #    dR[α,j] = sum_{β,γ} invA[α,β] * u[j,β] * Req[γ] * (δ_{β,γ} - l[j,β,γ])\n",
    "    eyeM = np.eye(M)\n",
    "    D = eyeM[None,:,:] - p['l']                # (N, M, M)\n",
    "    T = u[:,:,None] * Req[None,None,:] * D     # (N, M, M)\n",
    "    S = T.sum(axis=2)                          # (N, M)\n",
    "    dR = invA @ S.T                            # (M, N)\n",
    "\n",
    "    # 4) Effective interaction matrix α (N×N)\n",
    "    A_thing = u * (1 - lam)    # (N, M)\n",
    "    alpha   = A_thing @ dR     # (N, N)\n",
    "\n",
    "    # 5) Intrinsic LV growth rates r (N,)\n",
    "    O = A_thing @ Req          # (N,)\n",
    "    P = alpha @ Ceq            # (N,)\n",
    "    r_eff = O - P - m          # (N,)\n",
    "\n",
    "    # Build result dict with the exact keys eff_LV_jac will expect\n",
    "    result = {\n",
    "        'alpha': alpha,\n",
    "        'r':     r_eff,\n",
    "        'N':     N\n",
    "    }\n",
    "    if verbose:\n",
    "        result['dR'] = dR\n",
    "        result['A']  = A\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a0e7e4",
   "metadata": {
    "id": "13a0e7e4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LV_dx(x, t, p):\n",
    "    \"\"\"\n",
    "    Vectorized Lotka–Volterra RHS.\n",
    "\n",
    "    x : (N,) array of species abundances\n",
    "    p : dict with keys:\n",
    "        - 'r'     : (N,) intrinsic growth rates\n",
    "        - 'alpha' : (N, N) interaction matrix\n",
    "        - 'N'     : number of species (optional, not used here)\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    r     = p['r']        # (N,)\n",
    "    alpha = p['alpha']    # (N, N)\n",
    "\n",
    "    # compute interaction term: alpha @ x  → (N,)\n",
    "    interaction = alpha.dot(x)\n",
    "\n",
    "    # dx_i = x_i * (r_i + interaction_i)\n",
    "    dx = x * (r + interaction)\n",
    "\n",
    "    # zero out extremely small populations if you want the same “spacing” check\n",
    "    # e.g. dx[x <= np.spacing(x)] = 0.0\n",
    "\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c58ea8",
   "metadata": {
    "id": "d7c58ea8"
   },
   "outputs": [],
   "source": [
    "# from github Jacobian.jl, converted to python\n",
    "\n",
    "# this computes the Jacobian matrix\n",
    "# useful for downstream analysis on stability against perturbations\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eff_LV_params(p, sol, verbose=False):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of effective Lotka–Volterra parameters.\n",
    "    Returns a dict containing at least:\n",
    "      - 'alpha': the N×N interaction matrix\n",
    "      - 'r'    : the N-vector of intrinsic growth rates\n",
    "      - 'N'    : number of species\n",
    "    If verbose=True, also returns 'dR' and 'A' for debugging.\n",
    "    \"\"\"\n",
    "    M, N = p['M'], p['N']\n",
    "    # … your existing code that builds A, invA, dR, alpha, r_eff …\n",
    "\n",
    "    # --- here is the critical block to return the dict ---\n",
    "    result = {\n",
    "        'alpha': alpha,   # <— must be here\n",
    "        'r':     r_eff,   # <— must be here\n",
    "        'N':     N        # <— must be here\n",
    "    }\n",
    "    if verbose:\n",
    "        result['dR'] = dR\n",
    "        result['A']  = A\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e60917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "42e60917",
    "outputId": "d0cb9490-14c4-4d07-93b8-a6378d21d4ce"
   },
   "outputs": [],
   "source": [
    "# from github sim_frame.jl, converted to python. sim_frame.jl includes the previous 6 files.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.stats import dirichlet\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "# Custom MiCRM‐parameter‐generator functions (override defaults)\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "def F_m(N, M, kw):\n",
    "    \"\"\"\n",
    "    Temperature‐dependent mortality (maintenance) rates.\n",
    "    If 'tt' is in kw (an Nx2 array of TPC scalings), use the 2nd column;\n",
    "    otherwise fallback to a constant 0.2 for each species.\n",
    "    \"\"\"\n",
    "    if 'tt' in kw:\n",
    "        # kw['tt'] assumed shape (N,2)\n",
    "        return kw['tt'][:, 1] # this is the respiration/mortality part of 'tt', defined before. used to be temp_p_m.\n",
    "    else:\n",
    "        return np.full(N, 0.2)\n",
    "\n",
    "def F_rho(N, M, kw):\n",
    "    \"\"\"\n",
    "    Resource supply rates: constant ones by default.\n",
    "    \"\"\"\n",
    "    return np.ones(M)\n",
    "\n",
    "# could introduce temperature-dependence in this if we want to look at temp-dependent resource supply\n",
    "# same with omega below, seeing how temp affects loss/dilution of resources in specific ecosystems\n",
    "\n",
    "def F_omega(N, M, kw):\n",
    "    \"\"\"\n",
    "    Resource loss/dilution rates: zero by default.\n",
    "    \"\"\"\n",
    "    return np.zeros(M)\n",
    "\n",
    "def F_u(N, M, kw):\n",
    "    \"\"\"\n",
    "    Temperature‐scaled uptake matrix.\n",
    "      - Draws N Dirichlet(1,…,1) rows of length M (relative preference).\n",
    "      - Scales each row i by kw['tt'][i,0] if present, else by 2.5.\n",
    "    \"\"\"\n",
    "    # draw relative preferences\n",
    "    diri = np.stack([rng.dirichlet(np.ones(M)) for _ in range(N)], axis=0)\n",
    "\n",
    "    if 'tt' in kw:\n",
    "        u_sum = kw['tt'][:, 0] # this is the uptake part of 'tt', defined before. used to be temp_p_u.\n",
    "    else:\n",
    "        u_sum = np.full(N, 2.5)\n",
    "\n",
    "    # scale each row by its TPC magnitude\n",
    "    return diri * u_sum[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7f5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def eff_LV_jac(p_lv, sol, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Vectorized Jacobian of the effective GLV at equilibrium,\n",
    "    restricted to species with biomass > threshold.\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    alpha_full = p_lv['alpha']   # (N,N)\n",
    "    N_full     = p_lv['N']\n",
    "    bm         = sol.y[:N_full, -1]  # (N,)\n",
    "\n",
    "    # feasible indices\n",
    "    feasible = np.where(bm > threshold)[0]\n",
    "    if feasible.size == 0:\n",
    "        raise ValueError(\"No feasible species found!\")\n",
    "\n",
    "    # subset\n",
    "    C    = bm[feasible]                                # (n,)\n",
    "    alpha = alpha_full[np.ix_(feasible, feasible)]     # (n,n)\n",
    "\n",
    "    # Jacobian J_ij = alpha_ij * C_i\n",
    "    # can be written as diag(C) @ alpha\n",
    "    J = np.diag(C) @ alpha                             # (n,n)\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def MiCRM_jac(p, sol):\n",
    "    \"\"\"\n",
    "    Vectorized full Jacobian of the MiCRM system at equilibrium.\n",
    "    \"\"\"\n",
    "    N, M = p['N'], p['M']\n",
    "    lam   = p['lambda']   # (N,M)\n",
    "    l     = p['l']        # (N,M,M)\n",
    "    omega = p['omega']    # (M,)\n",
    "    m     = p['m']        # (N,)\n",
    "    u     = p['u']        # (N,M)\n",
    "\n",
    "    # Equilibrium state\n",
    "    state = sol.y[:, -1]\n",
    "    C     = state[:N]     # (N,)\n",
    "    R     = state[N:]     # (M,)\n",
    "\n",
    "    # --- Consumer–Consumer block (N×N) ---\n",
    "    cc_diag = -m + ((1 - lam) * u * R[None, :]).sum(axis=1)\n",
    "    CC = np.diag(cc_diag)\n",
    "\n",
    "    # --- Consumer–Resource block (N×M) ---\n",
    "    CR = C[:, None] * (1 - lam) * u  # (N,M)\n",
    "\n",
    "    # --- Resource–Resource block (M×M) ---\n",
    "    # P[i,α,β] = C[i]*u[i,α]*l[i,α,β]\n",
    "    P = C[:, None, None] * u[:, :, None] * l  # (N,M,M)\n",
    "    RR = P.sum(axis=0)                        # (M,M)\n",
    "\n",
    "    # Correct diagonal:\n",
    "    # diag_val[α] = ∑_i C[i]*u[i,α]*l[i,α,α]  = RR[α,α]\n",
    "    diag_val = np.diag(RR)                    # (M,)\n",
    "    # sub_diag[α] = ∑_i C[i]*u[i,α]\n",
    "    sub_diag = (C[:, None] * u).sum(axis=0)   # (M,)\n",
    "    # new diagonal = diag_val - sub_diag - omega\n",
    "    diag_rr = diag_val - sub_diag - omega     # (M,)\n",
    "    np.fill_diagonal(RR, diag_rr)\n",
    "\n",
    "    # --- Resource–Consumer block (M×N) ---\n",
    "    # term1 = -u * R  ; term2 = ∑_β u[i,β]*R[β]*l[i,β,α]\n",
    "    Q = u * R[None, :]                        # (N,M)\n",
    "    Ql = Q[:, :, None] * l                    # (N,M,M)\n",
    "    term2 = Ql.sum(axis=1)                    # (N,M)\n",
    "    RC = (term2 - Q).T                        # (M,N)\n",
    "\n",
    "    # Assemble\n",
    "    top    = np.hstack([CC, CR])              # (N, N+M)\n",
    "    bottom = np.hstack([RC, RR])              # (M, N+M)\n",
    "    return np.vstack([top, bottom])           # (N+M, N+M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### BELOW CODE SHOULD BE APPLICABLE FOR BOTH GLVM + MICRM EIGENVALUE CALCULATIONS #####\n",
    "\n",
    "def leading_eigenvalue(J):\n",
    "    \"\"\"\n",
    "    Compute the dominant eigenvalue (largest real part) of Jacobian matrix J.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(J)\n",
    "    return eigvals[np.argmax(np.real(eigvals))]\n",
    "\n",
    "\n",
    "def hermitian_part(J):\n",
    "    \"\"\"\n",
    "    Return the Hermitian (symmetric) part of a real matrix J: (J + J.T) / 2.\n",
    "    \"\"\"\n",
    "    return (J + J.T) / 2\n",
    "\n",
    "\n",
    "def leading_hermitian_eigenvalue(J):\n",
    "    \"\"\"\n",
    "    Compute the leading eigenvalue of the Hermitian part of J,\n",
    "    which indicates the reactivity of the system.\n",
    "    \"\"\"\n",
    "    H = hermitian_part(J)\n",
    "    # For symmetric H, use eigvalsh for efficiency and guaranteed real outputs\n",
    "    eigvals = np.linalg.eigvalsh(H)\n",
    "    return np.max(eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de27e4fb",
   "metadata": {
    "id": "de27e4fb"
   },
   "outputs": [],
   "source": [
    "# define functions for diversity\n",
    "\n",
    "\n",
    "##### Shannon diversity #####\n",
    "\n",
    "def shannon (abundance): # abundance should be an array (incl values for all consumers in system)\n",
    "    C_shannon = np.asarray(abundance, dtype=float) # convert into numpy arrays of floats\n",
    "\n",
    "    ### normalise to convert abundance into something on a scale of 0-1 ###\n",
    "\n",
    "    # total number of 'individuals' (add up all the relative abundances)\n",
    "    total_abundance = np.sum(C_shannon) # add up all the elements in this C_shannon array. total_abundance also a single value.\n",
    "\n",
    "    pi = C_shannon / total_abundance # pi is  an array. it now converts C_shannon into relative proportions, by dividing each element of C_shannon by the total_abundance value.\n",
    "\n",
    "    pi_lnpi = pi[pi > 0] * np.log(pi[pi > 0]) # this is pi * ln(pi). pi_lnpi should also be an array with N elements.\n",
    "    # keep only the pi > 0 ones. if pi = 0, will have issues with log. if pi = 0 it won't contribute to Shannon index anyway.\n",
    "\n",
    "    H = -np.sum(pi_lnpi)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "##### Bray-Curtis dissimilarity #####\n",
    "\n",
    "\n",
    "def bray_curtis_dissimilarity(G, M): # G = GLV, M = MiCRM\n",
    "\n",
    "    G_array = np.asarray(G, dtype=float) # convert G (abundance of each species predicted by GLV) into array\n",
    "    M_array = np.asarray(M, dtype=float) # convert M (abundance of each species predicted by MiCRM) into array\n",
    "\n",
    "    G_safe = np.where(G_array < 0, 0, G_array) # if any element (species) of the GLV-predicted array of abundances is less than 0, consider that 0\n",
    "    M_safe = np.where(M_array < 0, 0, M_array)\n",
    "\n",
    "    GM_dissimilarity = np.sum(np.abs(G_safe - M_safe)) / np.sum(G_safe + M_safe) # bray-curtis dissimilarity between GLV and MiCRM predictions\n",
    "\n",
    "    return GM_dissimilarity\n",
    "\n",
    "# also consider the case where total abundance is 0 (so would be dividing by 0)\n",
    "\n",
    "\n",
    "### Jaccard ###\n",
    "\n",
    "def jaccard_index(G, M, thresh=1e-8):\n",
    "    \"\"\"\n",
    "    Jaccard index J = |A ∩ B| / |A ∪ B| for presence/absence sets.\n",
    "    Presence := abundance > thresh.\n",
    "    Returns 1.0 if both sets are empty.\n",
    "    \"\"\"\n",
    "    G = np.asarray(G, dtype=float)\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    if G.shape != M.shape:\n",
    "        raise ValueError(\"G and M must have same shape\")\n",
    "    G_surv = G > thresh\n",
    "    M_surv = M > thresh\n",
    "    inter = np.logical_and(G_surv, M_surv).sum()\n",
    "    union = np.logical_or(G_surv, M_surv).sum()\n",
    "    return 1.0 if union == 0 else inter / union\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2047a505",
   "metadata": {
    "id": "2047a505"
   },
   "outputs": [],
   "source": [
    "### define functions for abundance deviation + species overlap between MiCRM and EGLV ###\n",
    "\n",
    "\"\"\"\n",
    "this is for each community simulation (N consumers), for each temperature\n",
    "to be incorporated into the later functions and loops (loop this function for each temp, then for each community simulation)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def err_eq_and_overlap(C_LV_eq, C_MiCRM_eq, thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "    - Mean log-ratio error in predicted abundances (GLV vs MiCRM)\n",
    "    - Number of overlapping survivors (identity-based overlap)\n",
    "\n",
    "    Only includes species that are predicted to survive by *both* models.\n",
    "\n",
    "    Inputs:\n",
    "    - C_LV_eq: GLV-predicted equilibrium consumer abundances (array-like)\n",
    "    - C_MiCRM_eq: MiCRM-predicted equilibrium consumer abundances (array-like)\n",
    "    - thresh: threshold below which species are considered extinct\n",
    "    - eps: small pseudocount to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    - equilibrium_error: mean log-ratio error between GLV and MiCRM (NaN if no shared survivors)\n",
    "    - overlap_count: number of species predicted to survive by both models\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to float arrays\n",
    "    C_LV = np.asarray(C_LV_eq, dtype=float)\n",
    "    C_Mi = np.asarray(C_MiCRM_eq, dtype=float)\n",
    "\n",
    "    # Identify survivors (boolean masks)\n",
    "    G_surv = C_LV > thresh\n",
    "    M_surv = C_Mi > thresh\n",
    "\n",
    "    # Overlap mask: species that survive in BOTH models\n",
    "    overlap_mask = G_surv & M_surv # element-wise check, so overlap_mask is true only if BOTH G_surv and M_surv are true for a species\n",
    "    overlap_count = np.sum(overlap_mask) # counts how many TRUE entries there are (size of intersection for survivors)\n",
    "\n",
    "    if overlap_count == 0:\n",
    "        return np.nan, 0  # No shared survivors, can't compute meaningful abundance deviation\n",
    "\n",
    "\n",
    "    # Log ratio of overlapping species\n",
    "    log_ratios = np.log(C_LV[overlap_mask] / C_Mi[overlap_mask])\n",
    "    equilibrium_error = np.mean(log_ratios)\n",
    "\n",
    "    return equilibrium_error, overlap_count\n",
    "\n",
    "\n",
    "# define function to produce the Err(t) function showing deviations over time\n",
    "\n",
    "\n",
    "def err_time_series(times, C_LV_traj, C_Mi_traj, thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Vectorized Err(t) and overlap count arrays.\n",
    "    \"\"\"\n",
    "    # Boolean mask of shape (N, T)\n",
    "    mask = (C_LV_traj > thresh) & (C_Mi_traj > thresh)\n",
    "\n",
    "    # Count survivors at each timepoint: shape (T,)\n",
    "    overlap_counts = mask.sum(axis=0)\n",
    "\n",
    "    # Compute all log‐ratios, then null out non‐overlap entries\n",
    "    log_ratios = np.log(C_LV_traj / C_Mi_traj)\n",
    "    log_ratios[~mask] = np.nan\n",
    "\n",
    "    # Mean across species yields Err(t): shape (T,)\n",
    "    err_t = np.nanmean(log_ratios, axis=0)\n",
    "\n",
    "    return err_t, overlap_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define function to produce the Errtraj, which integrates Err(t) and finds area under curve. represents trajectory deviation.\n",
    "\n",
    "def integrate_err(\n",
    "    times,  # 1D array of time‐points, shape (T,)\n",
    "    err_t   # 1D array of instantaneous errors, shape (T,)\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute the time‐averaged trajectory error\n",
    "      Err_traj = (1/(t_end - t_start)) * ∫ Err(t) dt\n",
    "    using the trapezoid rule on the discrete grid.\n",
    "    Returns a single float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask out any NaNs (where overlap = 0)\n",
    "    valid     = ~np.isnan(err_t)\n",
    "    t_valid   = times[valid]   # times where err_t is real\n",
    "    err_valid = err_t[valid]   # corresponding error values\n",
    "\n",
    "    # Need at least 2 points to do any integration\n",
    "    if valid.sum() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    # Numerically integrate Err(t) dt by the trapezoid rule:\n",
    "    integral = np.trapz(err_valid, x=t_valid)\n",
    "\n",
    "    # Divide by total time to get *average* error\n",
    "    duration = t_valid[-1] - t_valid[0]  # here, t_valid[-1] == t_end\n",
    "    return integral / duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeea832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2 versions of estimate_teq: \n",
    "\n",
    "- one is for estimating system-level teq for the purpose of trajectory deviation calculations.\n",
    "it estimates the system steady state time for BOTH MiCRM and GLVM \n",
    "this is useful for trajectory deviation calculations, because we integrate from 0 to teq, and divide by teq \n",
    "(could make this more granular and reduce to species-level teq - maybe in future work)\n",
    "\n",
    "- the other is for estimating system-level teq for the purpose of timescale separation calcs.\n",
    "it estimates the time it takes for EVERY consumer species in the MiCRM (not GLVM!) system \n",
    "to reach equilibrium. this is useful for timescale separation calculations, as we compare epsilon to the system teq. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def estimate_teq_traj(\n",
    "    times,    # 1D array of sampled times, shape (T,)\n",
    "    sol,      # MiCRM solution object from solve_ivp\n",
    "    sol_lv,   # GLV   solution object from solve_ivp\n",
    "    pT,       # parameter dict used for MiCRM_dxx\n",
    "    p_lv,     # parameter dict used for LV_dx\n",
    "    tol=1e-6, # threshold on derivative‐norm to call “flat”\n",
    "    window=5  # require this many consecutive below‐tol points\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Estimate the index j_eq corresponding to the time t_eq when BOTH\n",
    "    MiCRM and GLV trajectories have effectively reached steady‐state.\n",
    "    We do this by looking at the derivative norms ||dx/dt|| of each\n",
    "    system at each sampled time, taking their maximum, and finding the\n",
    "    first run of `window` consecutive samples all below `tol`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    j_eq : int\n",
    "      Index into `times` such that times[j_eq] is our estimated t_eq.\n",
    "      If no such run is found, returns T-1 (i.e., the last time index).\n",
    "    \"\"\"\n",
    "\n",
    "    T = times.size\n",
    "    # Prepare an array to store [||dx_mi||, ||dx_lv||] at each time\n",
    "    deriv_norms = np.empty((T, 2), dtype=float)\n",
    "\n",
    "    # Loop over each sampled time index\n",
    "    for j, t in enumerate(times):\n",
    "        # 1) MiCRM derivative at this time:\n",
    "        x_mi       = sol.y[:, j]                      # state vector length N+M\n",
    "        dx_mi      = MiCRM_dxx_numba_wrapper(x_mi, t, pT)            # compute RHS\n",
    "        deriv_norms[j, 0] = norm(dx_mi)                # Euclidean norm\n",
    "        \n",
    "        # 2)  GLV derivative at this time:\n",
    "        x_lv       = sol_lv.y[:, j]                   # state vector length N\n",
    "        dx_lv      = LV_dx(x_lv, t, p_lv)             # compute RHS\n",
    "        deriv_norms[j, 1] = norm(dx_lv)               # Euclidean norm\n",
    "\n",
    "    # At each time j, take the worst‐case (largest) of the two norms\n",
    "    combined_norm = np.max(deriv_norms, axis=1)       # shape (T,)\n",
    "\n",
    "    # Now find the first index j where combined_norm[j:(j+window)]\n",
    "    # are all < tol, i.e. both models stay flat for `window` steps\n",
    "    for j in range(0, T - window + 1):\n",
    "        # Check the next `window` points\n",
    "        if np.all(combined_norm[j : j + window] < tol):\n",
    "            # Once found, return j as the equilibrium index\n",
    "            return j\n",
    "\n",
    "    # If we never see `window` consecutive small‐derivative points, \n",
    "    # we assume equilibrium is at the final time\n",
    "    return T - 1\n",
    "\n",
    "\n",
    "def estimate_teq_timescale(\n",
    "    times,    # 1D array of sampled times, shape (T,)\n",
    "    sol,      # MiCRM solution object from solve_ivp\n",
    "    pT,       # parameter dict used for MiCRM_dxx\n",
    "    tol=1e-6, # threshold on derivative‐norm to call “flat”\n",
    "    window=5  # require this many consecutive below‐tol points\n",
    "    ):\n",
    "    \"\"\"\n",
    "    System‐level t_eq: the first time index at which *every* variable\n",
    "    in the MiCRM (all consumers + resources) has derivative magnitude \n",
    "    below tol for 'window' consecutive points.\n",
    "    \"\"\"\n",
    "    T = times.size\n",
    "    # will hold max |dx_i| over all i at each time\n",
    "    max_deriv = np.empty(T, dtype=float)\n",
    "\n",
    "    for j, t in enumerate(times):\n",
    "        x_mi  = sol.y[:, j]\n",
    "        dx_mi = MiCRM_dxx_numba_wrapper(x_mi, t, pT)\n",
    "        # track the worst‐behaved variable\n",
    "        max_deriv[j] = np.max(np.abs(dx_mi))\n",
    "\n",
    "    for j in range(T - window + 1):\n",
    "        if np.all(max_deriv[j : j + window] < tol):\n",
    "            return j\n",
    "    return T - 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90b4d80d",
   "metadata": {
    "id": "90b4d80d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Investigating the effect of temperature on higher-order interactions\n",
    "Hessian = second derivative\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "# 1) A Numba-friendly version of F_of_C_vec that takes each p-field as an argument.\n",
    "@njit\n",
    "def F_of_C_jit(C, N, M, u, lam, rho, omega, l, m):\n",
    "    \"\"\"\n",
    "    dC/dt with resources at equilibrium, Numaby-compiled.\n",
    "    C       : (N,)\n",
    "    u, lam  : (N, M)\n",
    "    rho,omega: (M,)\n",
    "    l       : (N, M, M)\n",
    "    m       : (N,)\n",
    "    \"\"\"\n",
    "    # Build A matrix\n",
    "    A = -np.diag(omega)\n",
    "    for i in range(N):\n",
    "        for b in range(M):\n",
    "            Wib = u[i, b] * C[i]\n",
    "            # add ∑_i l[i,b,α] * Wib\n",
    "            for a in range(M):\n",
    "                A[a, b] += l[i, a, b] * Wib\n",
    "    # subtract diagonal ∑_i u[i,α]*C[i]\n",
    "    for a in range(M):\n",
    "        s = 0.0\n",
    "        for i in range(N):\n",
    "            s += u[i, a] * C[i]\n",
    "        A[a, a] -= s\n",
    "\n",
    "    # solve A @ R_star = rho\n",
    "    R_star = np.linalg.solve(A, rho)\n",
    "\n",
    "    # compute net growth\n",
    "    net = np.empty(N)\n",
    "    for i in range(N):\n",
    "        s = 0.0\n",
    "        for a in range(M):\n",
    "            s += (1 - lam[i, a]) * u[i, a] * R_star[a]\n",
    "        net[i] = s - m[i]\n",
    "\n",
    "    # return dC/dt\n",
    "    dC = np.empty(N)\n",
    "    for i in range(N):\n",
    "        dC[i] = C[i] * net[i]\n",
    "    return dC\n",
    "\n",
    "# 2) Numba-accelerated Hessian norm, calling the jitted F_of_C_jit\n",
    "@njit(parallel=True)\n",
    "def compute_hessian_norm_nb(C_eq, N, M, u, lam, rho, omega, l, m, eps=1e-6):\n",
    "    H2_sum = 0.0\n",
    "    for j in prange(N):\n",
    "        for k in range(N):\n",
    "            # build perturbation vectors\n",
    "            C_pp = C_eq.copy(); C_pp[j] += eps; C_pp[k] += eps\n",
    "            C_pm = C_eq.copy(); C_pm[j] += eps; C_pm[k] -= eps\n",
    "            C_mp = C_eq.copy(); C_mp[j] -= eps; C_mp[k] += eps\n",
    "            C_mm = C_eq.copy(); C_mm[j] -= eps; C_mm[k] -= eps\n",
    "\n",
    "            # central differences\n",
    "            F_pp = F_of_C_jit(C_pp, N, M, u, lam, rho, omega, l, m)\n",
    "            F_pm = F_of_C_jit(C_pm, N, M, u, lam, rho, omega, l, m)\n",
    "            F_mp = F_of_C_jit(C_mp, N, M, u, lam, rho, omega, l, m)\n",
    "            F_mm = F_of_C_jit(C_mm, N, M, u, lam, rho, omega, l, m)\n",
    "\n",
    "            for i in range(N):\n",
    "                d2 = (F_pp[i] - F_pm[i] - F_mp[i] + F_mm[i])\n",
    "                H2_sum += (d2 * d2) / (4 * eps * eps)\n",
    "\n",
    "    return np.sqrt(H2_sum)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1821bb89",
   "metadata": {
    "id": "1821bb89"
   },
   "outputs": [],
   "source": [
    "\n",
    "def timescale_separation_full(J_full, N):\n",
    "    \"\"\"\n",
    "    Given the full (N+M)x(N+M) MiCRM Jacobian J_full and number of consumers N,\n",
    "    returns (tau_C, tau_R, epsilon) by eigenmode classification.\n",
    "    \"\"\"\n",
    "    # 1) eigendecompose\n",
    "    eigvals, eigvecs = np.linalg.eig(J_full)\n",
    "    re_times = 1.0 / np.abs(np.real(eigvals))\n",
    "\n",
    "    # 2) classify modes by where v has more weight\n",
    "    #    sum |v[i]| over consumers vs. resources\n",
    "    weights = np.abs(eigvecs)\n",
    "    cons_weight = weights[:N, :].sum(axis=0)\n",
    "    res_weight  = weights[N:, :].sum(axis=0)\n",
    "\n",
    "    cons_mask = cons_weight >= res_weight\n",
    "    res_mask  = ~cons_mask\n",
    "\n",
    "    # 3) select timescales\n",
    "    #    - consumers: fastest return (smallest tau)\n",
    "    #    - resources: slowest return (largest tau)\n",
    "    if not np.any(cons_mask) or not np.any(res_mask):\n",
    "        raise ValueError(\"No pure consumer or resource modes found!\")\n",
    "    tau_C = np.min(re_times[cons_mask])\n",
    "    tau_R = np.max(re_times[res_mask])\n",
    "\n",
    "    # 4) separation\n",
    "    epsilon = tau_C / tau_R\n",
    "    return tau_C, tau_R, epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this calculates equilibrium values (C*, R*) + trajectory errors \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ─── equilibrium_and_traj.py ─────────────────────────────────────────────\n",
    "# Calculates equilibrium (C*, R*) and trajectory errors for MiCRM and GLV\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "\n",
    "# ─── Output directories ───────────────────────────────\n",
    "outdir = \"output\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "paramfile = os.path.join(outdir, \"structural_params_all.pkl\")  # unified pickle output\n",
    "\n",
    "# ─── Simulation parameters ────────────────────────────\n",
    "N = 10\n",
    "M = 5\n",
    "L = np.full(N, 0.3)\n",
    "x0 = np.concatenate([np.full(N, 0.1), np.full(M, 1)])\n",
    "temp_vals = np.linspace(273.15, 273.15 + 30, 3)\n",
    "rho_t = np.array([-0.5, -0.5])\n",
    "Tr = 273.15 + 10\n",
    "Ed = 3.5\n",
    "\n",
    "# ─── Per‐replicate function ───────────────────────────\n",
    "def run_single_full_raw(replicate_id):\n",
    "    structural = generate_params(\n",
    "        N, M,\n",
    "        f_u=def_u, f_m=def_m, f_rho=def_rho,\n",
    "        f_omega=def_omega, f_l=def_l,\n",
    "        L=L,\n",
    "        T=273.15, rho_t=rho_t, Tr=Tr, Ed=Ed,\n",
    "        rng=default_rng(111 + replicate_id)\n",
    "    )\n",
    "\n",
    "    raw_rows = []\n",
    "    traj_rows = []\n",
    "\n",
    "    for T in temp_vals:\n",
    "        # ─── Temperature-scaled params ───────────────\n",
    "        temp_p, _, _, _ = temp_trait(N, {\n",
    "            'T': T, 'Tr': Tr, 'Ed': Ed, 'rho_t': rho_t, 'L': L\n",
    "        })\n",
    "        \n",
    "        pT = {\n",
    "            **structural,\n",
    "            'u': structural['u'] * temp_p[:, 0][:, None],\n",
    "            'm': temp_p[:, 1],\n",
    "            'lambda': np.sum(structural['l'], axis=2),\n",
    "            'T': T\n",
    "        }\n",
    "\n",
    "        # ─── Solve models ────────────────────────────\n",
    "        t_eval = np.linspace(0, 1000, 500)\n",
    "        sol = solve_ivp(lambda t, y: MiCRM_dxx_numba_wrapper(t, y, pT),\n",
    "                        (0, 1000), x0, method='LSODA',\n",
    "                        rtol=1e-4, atol=1e-7, t_eval=t_eval)\n",
    "\n",
    "        p_lv = eff_LV_params(pT, sol, verbose=False)\n",
    "        sol_lv = solve_ivp(lambda t, y: LV_dx(y, t, p_lv),\n",
    "                           (0, 1000), sol.y[:N, 0], method='LSODA',\n",
    "                           rtol=1e-4, atol=1e-7, t_eval=t_eval)\n",
    "\n",
    "        # ─── Final equilibrium values ────────────────\n",
    "        C_eq_mi = sol.y[:N, -1]\n",
    "        R_eq_mi = sol.y[N:, -1]\n",
    "        C_eq_lv = sol_lv.y[:N, -1]\n",
    "\n",
    "        raw = {\n",
    "            'replicate': replicate_id,\n",
    "            'T_K':       T,\n",
    "            'T_C':       T - 273.15\n",
    "        }\n",
    "        for i in range(N):\n",
    "            raw[f'Cmi_{i}'] = C_eq_mi[i]\n",
    "            raw[f'Clv_{i}'] = C_eq_lv[i]\n",
    "        for j in range(M):\n",
    "            raw[f'Rmi_{j}'] = R_eq_mi[j]\n",
    "\n",
    "        raw_rows.append(raw)\n",
    "\n",
    "        # ─── Trajectory metrics ─────────────────────\n",
    "        times = sol.t\n",
    "        err_t, _ = err_time_series(times, sol_lv.y[:N], sol.y[:N])\n",
    "        j_eq = estimate_teq_traj(times, sol, sol_lv, pT, p_lv,\n",
    "                                 tol=1e-6, window=5)\n",
    "        times_crop = times[: j_eq + 1]\n",
    "        err_crop = err_t[: j_eq + 1]\n",
    "        ErrTraj = integrate_err(times_crop, err_crop)\n",
    "\n",
    "        j_eq_mi = estimate_teq_timescale(sol.t, sol, pT)\n",
    "        t_eq_mi = sol.t[j_eq_mi]\n",
    "\n",
    "        traj_rows.append({\n",
    "            'replicate': replicate_id,\n",
    "            'T_K':       T,\n",
    "            'T_C':       T - 273.15,\n",
    "            't_eq':      times[j_eq],\n",
    "            'ErrTraj':   ErrTraj,\n",
    "            't_eq_mi':   t_eq_mi\n",
    "        })\n",
    "\n",
    "    return replicate_id, structural, raw_rows, traj_rows\n",
    "\n",
    "# ─── Main loop ───────────────────────────────────────\n",
    "if __name__ == '__main__':\n",
    "    full_ids = list(range(1, 101))\n",
    "    with mp.Pool(8) as pool:\n",
    "        all_results = pool.map(run_single_full_raw, full_ids)\n",
    "\n",
    "    # ─── Unpack results ──────────────────────────────\n",
    "    all_struct = {rep_id: struct for rep_id, struct, _, _ in all_results}\n",
    "    raw_full   = [r for _, _, rows, _ in all_results for r in rows]\n",
    "    traj_full  = [t for _, _, _, rows in all_results for t in rows]\n",
    "\n",
    "    # ─── Save outputs ────────────────────────────────\n",
    "    pd.DataFrame(raw_full).to_csv(os.path.join(outdir, 'raw_eq_100.csv'), index=False)\n",
    "    pd.DataFrame(traj_full).to_csv(os.path.join(outdir, 'traj_100.csv'), index=False)\n",
    "\n",
    "    with open(paramfile, 'wb') as f:\n",
    "        pickle.dump(all_struct, f)\n",
    "\n",
    "    print(\"Done:\\n • raw_eq_100.csv\\n • traj_100.csv\\n • structural_params_all.pkl\")\n",
    "\n",
    "   \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-WMYeCZFTtA-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WMYeCZFTtA-",
    "outputId": "0a7895dc-460f-4f02-a9ec-16a1336042c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics → metrics_70.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "computing metrics based on the equilibrium values C* and R* obtained from previous csv file. \n",
    "e.g. Jacobian, Hermitian, Hessian, diversity, equilibrium deviation, etc. \n",
    "This metrics output file will contain all the info needed for plotting straightaway. \n",
    "\n",
    "This metrics file includes parameter generation etc, which should be the EXACT SAME 100 COMMUNITIES as the previous csv files. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Loads equilibria and structural parameters, computes metrics, saves to CSV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ─── Config paths ───────────────────────────────────\n",
    "RAW_EQ_CSV   = 'output/raw_eq_100.csv'\n",
    "TRAJ_CSV     = 'output/traj_100.csv'\n",
    "PICKLE_FILE  = 'output/structural_params_all.pkl'\n",
    "OUT_MET_CSV  = 'output/metrics_100.csv'\n",
    "\n",
    "N = 100\n",
    "M = 50\n",
    "\n",
    "# ─── Load data ──────────────────────────────────────\n",
    "df_eq   = pd.read_csv(RAW_EQ_CSV)\n",
    "df_traj = pd.read_csv(TRAJ_CSV)\n",
    "\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "    structural_all = pickle.load(f)\n",
    "\n",
    "reps  = sorted(df_eq['replicate'].unique())\n",
    "temps = sorted(df_eq['T_K'].unique())\n",
    "\n",
    "# ─── Precompute temp scalars ───────────────────────\n",
    "temp_scalars = {\n",
    "    T_K: temp_trait(N, {'T':T_K, 'Tr':273.15+10, 'Ed':3.5, 'rho_t':np.array([0.0,0.0]), 'L':np.full(N,0.3)})[0]\n",
    "    for T_K in temps\n",
    "}\n",
    "\n",
    "class SolDummy: pass\n",
    "\n",
    "records = []\n",
    "for rep in reps:\n",
    "    structural = structural_all[rep]\n",
    "\n",
    "    df_rep_eq   = df_eq[df_eq['replicate'] == rep].set_index('T_K')\n",
    "    df_rep_traj = df_traj[df_traj['replicate'] == rep].set_index('T_K')\n",
    "\n",
    "    for T_K in temps:\n",
    "        row_eq = df_rep_eq.loc[T_K]\n",
    "        Cmi    = row_eq.filter(like='Cmi_').values\n",
    "        Clv    = row_eq.filter(like='Clv_').values\n",
    "        Rmi    = row_eq.filter(like='Rmi_').values\n",
    "        T_C    = T_K - 273.15\n",
    "        t_eq_mi = df_rep_traj.loc[T_K, 't_eq_mi']\n",
    "\n",
    "        ErrEqAb, overlap = err_eq_and_overlap(Clv, Cmi)\n",
    "        jacc             = jaccard_index(Clv, Cmi, thresh=1e-6)\n",
    "        sh_mi, sh_lv     = shannon(Cmi), shannon(Clv)\n",
    "        bc               = bray_curtis_dissimilarity(Clv, Cmi)\n",
    "\n",
    "        temp_p = temp_scalars[T_K]\n",
    "        pT = {\n",
    "            **structural,\n",
    "            'u': structural['u'] * temp_p[:, 0][:, None],\n",
    "            'm': temp_p[:, 1],\n",
    "            'lambda': structural['l'].sum(axis=2),\n",
    "            'T': T_K\n",
    "        }\n",
    "\n",
    "        sol_eq       = SolDummy()\n",
    "        sol_eq.y     = np.concatenate([Cmi, Rmi])[:, None]\n",
    "        sol_eq.t     = np.array([t_eq_mi])\n",
    "\n",
    "        p_lv         = eff_LV_params(pT, sol_eq, verbose=False)\n",
    "        J_glv        = eff_LV_jac(p_lv, sol_eq)\n",
    "        stab_glv     = leading_eigenvalue(J_glv)\n",
    "        react_glv    = leading_hermitian_eigenvalue(J_glv)\n",
    "\n",
    "        J_mic        = MiCRM_jac(pT, sol_eq)\n",
    "        stab_mic     = leading_eigenvalue(J_mic)\n",
    "        react_mic    = leading_hermitian_eigenvalue(J_mic)\n",
    "\n",
    "        tau_C, tau_R, eps = timescale_separation_full(J_mic, N)\n",
    "        log10_eps_teq     = np.log10(eps / t_eq_mi)\n",
    "\n",
    "        hnorm = compute_hessian_norm_nb(\n",
    "            Cmi, N, M,\n",
    "            pT['u'], pT['lambda'], pT['rho'],\n",
    "            pT['omega'], pT['l'], pT['m'],\n",
    "            eps=1e-6\n",
    "        )\n",
    "\n",
    "        comm  = J_mic @ J_mic.T - J_mic.T @ J_mic\n",
    "        nnorm = np.linalg.norm(comm, ord='fro')\n",
    "\n",
    "        records.append({\n",
    "            'replicate':       rep,\n",
    "            'T_K':             T_K,\n",
    "            'T_C':             T_C,\n",
    "            'ErrEqAb':         ErrEqAb,\n",
    "            'overlap':         overlap,\n",
    "            'jaccard':         jacc,\n",
    "            'shannon_mi':      sh_mi,\n",
    "            'shannon_lv':      sh_lv,\n",
    "            'bray_curtis':     bc,\n",
    "            'stab_glv':        stab_glv,\n",
    "            'stab_mi':         stab_mic,\n",
    "            'abs_stab_err':    abs(stab_glv - stab_mic),\n",
    "            'react_glv':       react_glv,\n",
    "            'react_mi':        react_mic,\n",
    "            'abs_react_err':   abs(react_glv - react_mic),\n",
    "            'tau_C':           tau_C,\n",
    "            'tau_R':           tau_R,\n",
    "            'epsilon':         eps,\n",
    "            't_eq_mi':         t_eq_mi,\n",
    "            'log10_eps_t_eq':  log10_eps_teq,\n",
    "            'hessian_norm':    hnorm,\n",
    "            'non_normality':   nnorm\n",
    "        })\n",
    "\n",
    "# ─── Save final metrics ─────────────────────────────\n",
    "pd.DataFrame(records).to_csv(OUT_MET_CSV, index=False)\n",
    "print(f\"Saved metrics → {OUT_MET_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
