{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from github temp.jl, converted to python\n",
    "\n",
    "# this section is about incorporating temperature dependence \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "rng = default_rng(111) # random number generator \n",
    "\n",
    "\n",
    "# note that the u(T) and m(T) equation have the same structure, except the parameters are different. \n",
    "# both of the equations involve parameters like:\n",
    "# B0 (base rate), E (activation energy), k (Boltzmann constant), Ed (deactivation energy), Tp (peak performance temperature), and Tr (reference temperature) \n",
    "# first, we want to generate random parameters for each consumer (as mentioned above), like B, E, Tp\n",
    "# and for each of these temp-dependent parameters, we want to generate one for uptake u(T) and one for respiration m(T) \n",
    "# they're not actually fully random, as we do define mean and variance for teh parameters, and draw from a multivariate normal distribution \n",
    "\n",
    "\n",
    "\n",
    "def randtemp_param(N, kw):\n",
    "    \"\"\"\n",
    "    Generate random temperature-dependent trait parameters for consumers.\n",
    "    \n",
    "    Returns:\n",
    "        B: base rates (N x 2)\n",
    "        E: activation energies (N x 2)\n",
    "        Tp: peak temperatures (N x 2)\n",
    "    \"\"\"\n",
    "    L = kw['L'] # leakage \n",
    "    rho_t = kw['rho_t']\n",
    "\n",
    "    L_v = np.mean(L)\n",
    "    B0_m = -1.4954 # I think this is mortality / respiration rate \n",
    "    B0_CUE = 0.1953 \n",
    "    B0_u = np.log(np.exp(B0_m) / (1 - L_v - B0_CUE)) # I think this is uptake rate. dependent on carbon use efficiency (CUE) and leakage rate\n",
    "    \n",
    "    B0 = np.array([B0_u, B0_m]) # B0 is a vector of base rates for uptake and respiration\n",
    "    B0_var = 0.17 * np.abs(B0) # variance of base rates, 0.17 is a scaling factor\n",
    "    E_mean = np.array([0.8146, 0.5741]) # mean activation energies for uptake and respiration\n",
    "    E_var = 0.1364 * E_mean # variance of activation energies, 0.1364 is a scaling factor\n",
    "    cov_xy = rho_t * np.sqrt(B0_var * E_var) # covariance between base rates and activation energies, rho_t is the correlation coefficient\n",
    "\n",
    "    cov_u = np.array([[B0_var[0], cov_xy[0]], [cov_xy[0], E_var[0]]]) # covariance matrix for uptake\n",
    "    cov_m = np.array([[B0_var[1], cov_xy[1]], [cov_xy[1], E_var[1]]]) # covariance matrix for respiration\n",
    "\n",
    "    allu = multivariate_normal.rvs(mean=[B0[0], E_mean[0]], cov=cov_u, size=N).T # draw random samples from multivariate normal distribution for uptake\n",
    "    allm = multivariate_normal.rvs(mean=[B0[1], E_mean[1]], cov=cov_m, size=N).T # draw random samples from multivariate normal distribution for respiration\n",
    "\n",
    "    B = np.column_stack((np.exp(allu[0]), np.exp(allm[0]))) # exponentiate the base rates to get the actual values\n",
    "    E = np.column_stack((allu[1], allm[1])) # activation energies are already in the correct form\n",
    "\n",
    "    Tpu = 273.15 + rng.normal(35, 5, N) # draw random peak temperatures for uptake from a normal distribution with mean 35 and std 5\n",
    "    Tpm = Tpu + 3 # peak temperature for respiration is 3 degrees higher than for uptake\n",
    "    Tp = np.column_stack((Tpu, Tpm)) # combine the peak temperatures into a single array \n",
    "\n",
    "    return B, E, Tp\n",
    "\n",
    "\n",
    "# randtemp_param_test = randtemp_param(3, {'L': 0.4, 'rho_t': -0.75}) \n",
    "# print(randtemp_param_test) \n",
    "# this works - produces 2D arrays \n",
    "\n",
    "\n",
    "# now we have generated parameters (generating B, E, Tp) for each consumer \n",
    "# we did this by drawing from a multivariate normal distribution, with some constraints like mean, variance, correlation that we defined \n",
    "# now that the parameters are generated, we can incorporate them into the Arrhenius equation to calculate the temperature-dependent trait values \n",
    "# since uptake u(T) and respiration m(T) both depend on these parameters like B, E, Tp (which we have now defined) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def temp_trait(N, kw):\n",
    "    \"\"\"\n",
    "    Compute temperature-dependent trait scaling based on an Arrhenius-like thermal performance curve.\n",
    "    \n",
    "    Arguments:\n",
    "        N: number of consumers\n",
    "        kw: dict containing temperature parameters (T, Tr, Ed, L, rho_t)\n",
    "\n",
    "    Returns:\n",
    "        temp_p: temperature-scaled trait values (vector of size N)\n",
    "        B, E: base rates and activation energies (N x 2)\n",
    "        Tp: peak temperatures for uptake and respiration (N x 2)\n",
    "    \"\"\"\n",
    "    k = 0.0000862  # Boltzmann constant. used in exponential term of the Arrhenius equation\n",
    "    T = kw['T']\n",
    "    Tr = kw['Tr'] \n",
    "    Ed = kw['Ed']\n",
    "\n",
    "    B, E, Tp = randtemp_param(N, kw) # draw random base rates (B), activation energies (E), and peak temperatures (Tp) for each consumer. \n",
    "    # we are using the previously defined function to generate these parameters  \n",
    "\n",
    "    # Arrhenius function with high-temp deactivation\n",
    "\n",
    "    # uptake rate u(T)\n",
    "    temp_p_u = B[:, 0] * np.exp((-E[:, 0] / k) * ((1 / T) - (1 / Tr))) / \\\n",
    "              (1 + (E[:, 0] / (Ed - E[:, 0])) * np.exp(Ed / k * (1 / Tp[:, 0] - 1 / T)))\n",
    "    \n",
    "    # respiration rate m(T) \n",
    "    temp_p_m = B[:, 1] * np.exp((-E[:, 1] / k) * ((1 / T) - (1 / Tr))) / \\\n",
    "              (1 + (E[:, 1] / (Ed - E[:, 1])) * np.exp(Ed / k * (1 / Tp[:, 1] - 1 / T)))\n",
    "\n",
    "    temp_p = np.column_stack((temp_p_u, temp_p_m))  # shape (N,2)\n",
    "    \n",
    "    return temp_p, B, E, Tp\n",
    "\n",
    "\n",
    "# temp_trait_test = temp_trait(3, {'T': 273.15 + 10, 'Tr': 273.15 + 10, 'Ed': 3.5, 'L': 0.4, 'rho_t': -0.75})\n",
    "# print(temp_trait_test)\n",
    "\n",
    "\n",
    "# this temp_trait function uses the previous randtemp_param function to generate parameters for TPC, which are B, E, Tp \n",
    "# and then it uses these parameters to calculate the temperature-dependent trait values (u and m, which are uptake and respiration)  \n",
    "# it does this by putting these parameters into the Arrhenius-like TPC equations u(T) and m(T) \n",
    "# the inputs are N (number of consumers) and kw (a dictionary of parameters like T, Tr, Ed, L, rho_t)\n",
    "# and the outputs are the temp_p (temperature-scaled trait values), B (base rates), E (activation energies), and Tp (peak temperatures)\n",
    "# we could define some temperature T, and the temp_p functions would calculate the resulting u and m for that temperature \n",
    "# so using this, we can compare e.g. low vs high temperatures by defining different T values \n",
    "# and potentially make continuous graphs of temperature vs growth rate or something, and use that to inform timescale separation / GLV accuracy \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42b142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from github micrm_params.jl, converted to python \n",
    "\n",
    "# in the previous block of code, we generated temperature-dependent parameters\n",
    "# but those parameters were just for the TPC \n",
    "# we also need to produce the other parameters for MiCRM \n",
    "# so this block of code focuses on defining parameters for MiCRM \n",
    "# and we later use these parameters to run simulations etc \n",
    "\n",
    "\n",
    "# kw is a dictionary carrying all the model inputs and intermediate results (temp, TPC scaling, leakage scalar, etc) \n",
    "# we like to include it (like in def_m) even though it's not needed for the default version\n",
    "# so that any generator function can just pull out what it needs from kw \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Default parameter-generating functions\n",
    "def def_m(N, M, kw):\n",
    "    # Respiration (mortality) rates: ones vector of length N.\n",
    "    return np.ones(N)\n",
    "\n",
    "def def_rho(N, M, kw):\n",
    "    # Resource-specific supply rates: ones vector of length M.\n",
    "    # this was originally resource loss rate, but i changed it to supply so it makes sense with the equation \n",
    "    return np.ones(M)\n",
    "\n",
    "def def_omega(N, M, kw):\n",
    "    # Resource loss rates: ones vector of length M.\n",
    "    return np.ones(M)\n",
    "\n",
    "\n",
    "# the above code for def_m and def_rho are just returning vectors of ones, length N for respiration (m) and length M for supply (rho). \n",
    "# these are like the 'default' respiration/mortality rates, and default resource supply rates. def = default \n",
    "# depends on number of consumers N and number of resources M \n",
    "# later we can add temperature dependence, using functions we defined in previous section of code \n",
    "\n",
    "\n",
    "\n",
    "def def_u(N, M, kw):\n",
    "    # Uptake matrix: each of N rows is drawn from a symmetric Dirichlet of dimension M.\n",
    "    # this establishes the structure of the uptake function - it's a matrix of N x M \n",
    "    # unlike others like respiration (m) which is just a vector of length N \n",
    "    # numpy.random.dirichlet uses concentration parameters of length M\n",
    "    return np.random.dirichlet(alpha=np.ones(M), size=N)\n",
    "\n",
    "\n",
    "# the 'u' is an N x M matrix. each of the rows are sampled from a Dirichlet distribution. \n",
    "# each consumer is a row, and each resource is a column. \n",
    "# each consumer's uptake across resources (sum of a row) is 1. but this is randomly allocated among M resources. \n",
    "# so i guess the uptake is not absolute units but it's relative to other resources? \n",
    "# also i think this describes uptake preference (so how much of a certain resource it uptakes vs another one, relatively), not uptake rate which can be temp-dependent (as previously defined) \n",
    "\n",
    "\n",
    "def def_l(N, M, L):\n",
    "    # Leakage-transformation tensor: shape (N, M, M). For each consumer i and resource alpha,\n",
    "    # draw an M-vector from Dirichlet and scale by L[i].\n",
    "    l = np.zeros((N, M, M))\n",
    "    phi = np.ones(M)\n",
    "    for i in range(N):\n",
    "        for alpha in range(M):\n",
    "            draw = np.random.dirichlet(alpha=phi)\n",
    "            l[i, alpha, :] = draw * L[i]\n",
    "    return l\n",
    "\n",
    "# again, i think leakage is relative units (arbitrary units or sth)\n",
    "# more about the 'preference' in terms of which resources are leaked \n",
    "\n",
    "\n",
    "# this is the N x M x M leakage tensor. for each consumer i and resource index alpha, we are drawing an M-vector from a Dirichlet distribution, and scaling it by L[i].\n",
    "# this is the leakage matrix for each consumer-resource pair. \n",
    "# from the paper: \n",
    "# L encodes each strain's metabolic network. L is the leakage-transformation tensor. L determines how consumed substrates are leaked / metabolically transformed \n",
    "# so i think for each strain N + each resource M, there is a whole vector (length M) of what resources that this one coudl be leaked as or transformed into. \n",
    "\n",
    "# these previous functions are 'default' functions for generating parameters\n",
    "# they create simple placeholder values like vectors of 1s, or random Dirichlet distributions \n",
    "# these functions can be overridden (e.g. after we add temp dependence)\n",
    "\n",
    "# next, in generate_params, we will add temperature dependence to the parameters \n",
    "\n",
    "def generate_params(N,\n",
    "                     M,\n",
    "                     f_m=def_m,\n",
    "                     f_rho=def_rho,\n",
    "                     f_omega=def_omega,\n",
    "                     f_u=def_u,\n",
    "                     f_l=def_l,\n",
    "                     **kwargs):\n",
    "    \n",
    "    # f_m = def_m means that if we don't provide a function for f_m, it will use the default one (def_m)\n",
    "    # this is why we had to define the default functions earlier \n",
    "    # **kwargs is any other keyword arguments (not directly specified as function inputs here, but bundled together in dictionary)\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate temperature-dependent MiCRM parameters.\n",
    "\n",
    "    Parameters:\n",
    "        N, M       : integers, number of consumers and resources\n",
    "        f_m, f_rho, f_omega, f_u, f_l : functions to generate m, rho, omega, u, l\n",
    "        kwargs     : other keyword arguments (e.g., T, rho_t, Tr, Ed, L)\n",
    "\n",
    "    Returns:\n",
    "        params : dict with keys 'N', 'M', 'u', 'm', 'l', 'rho', 'omega', 'lambda',\n",
    "                 plus temperature traits and any extra kwargs\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy kwargs for internal use\n",
    "    kw = dict(kwargs)\n",
    "\n",
    "    # Temperature-dependent traits, as defined in previous code block \n",
    "    tt, B, E, Tp = temp_trait(N, kw) # according to previously defined temp_trait function \n",
    "    # tt is the first output of the temp_trait function, which is temp_p\n",
    "\n",
    "    \n",
    "    kw['tt'] = tt\n",
    "   \n",
    "    # in this section above:\n",
    "    # we plug in N and kw (kw is a dictionary of parameters), both of which are inputs to the overall generate_params function\n",
    "    # and using a previously defined function temp_trait, we get parameters for temperature dependence\n",
    "    # the parameters generated include temp_p_u and temp_p_m \n",
    "    # now that we've generated temp_p_u and temp_p_m, we want to store them into kw dictionary \n",
    "\n",
    "    # Generate consumer parameters\n",
    "    m = f_m(N, M, kw) # Respiration (mortality) rates\n",
    "    u = f_u(N, M, kw) # Uptake matrix\n",
    "\n",
    "    # this section above:\n",
    "    # f_m and f_u generate the final parameters for uptake and mortality \n",
    "    # we haven't defined a custom function for f_m or f_u, so it will use the default ones we defined earlier\n",
    "    # so here, it's basically same as m = def_m(N, M, kw) and u = def_u(N, M, kw)\n",
    "\n",
    "\n",
    "    # Leakage-transformation tensor\n",
    "    L = kw.get('L')         # Expect L in kwargs\n",
    "    l = f_l(N, M, L)      # Leakage-transformation \n",
    "\n",
    "    # Total leakage per consumer-resource pair\n",
    "    lambda_ = np.sum(l, axis=2)  # shape (N, M)\n",
    "\n",
    "    # Resource parameters\n",
    "    rho = f_rho(N, M, kw) \n",
    "    omega = f_omega(N, M, kw)\n",
    "\n",
    "    # Assemble base parameter dict\n",
    "    params = {\n",
    "        'N': N,\n",
    "        'M': M,\n",
    "        'u': u,\n",
    "        'm': m,\n",
    "        'l': l,\n",
    "        'rho': rho,\n",
    "        'omega': omega,\n",
    "        'lambda': lambda_,\n",
    "        'L': L,\n",
    "        'B': B,\n",
    "        'E': E,\n",
    "        'Tp': Tp,\n",
    "        'tt': tt\n",
    "    }\n",
    "    # Merge in any extra user-supplied kwargs\n",
    "    params.update(kwargs) \n",
    "\n",
    "    return params # so it shows the base parameter dictionary we defined within this function \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test generate_params just to show how it works:\n",
    "# given N, M, and other parameters like temperature, it generates parameters for MiCRM \n",
    "# and there is an element of stochasticity, because the community assemblies are random \n",
    "\n",
    "# params = generate_params(\n",
    "#     N=10,\n",
    "#     M=5,\n",
    "#     T=310,\n",
    "#     Tr=275,\n",
    "#     Ed=2,\n",
    "#     L=np.random.uniform(0.1, 0.5, size=N),\n",
    "#     rho_t=1\n",
    "# )\n",
    "\n",
    "# print (params)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2cba08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from github dx_v2.jl, converted to python\n",
    "\n",
    "\n",
    "# now that we've generated parameters, we can write the actual MiCRM model \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def MiCRM_dxx_jit(x, t, N, M, u, l, rho, omega, m):\n",
    "    dx = np.zeros(N + M)\n",
    "    # consumer dynamics\n",
    "    for i in range(N):\n",
    "        dx[i] = -m[i] * x[i]\n",
    "        for alpha in range(M):\n",
    "            res_idx = N + alpha\n",
    "            uptake = x[i] * x[res_idx] * u[i, alpha]\n",
    "            dx[i] += uptake\n",
    "            for beta in range(M):\n",
    "                dx[i] -= uptake * l[i, alpha, beta]\n",
    "    # resource dynamics\n",
    "    for alpha in range(M):\n",
    "        idx = N + alpha\n",
    "        dx[idx] = rho[alpha] - omega[alpha] * x[idx]\n",
    "        for i in range(N):\n",
    "            dx[idx] -= u[i, alpha] * x[idx] * x[i]\n",
    "            for beta in range(M):\n",
    "                dx[idx] += x[N + beta] * x[i] * u[i, beta] * l[i, beta, alpha]\n",
    "    return dx\n",
    "\n",
    "# In solve_ivp, use a wrapper to pass jit arguments:\n",
    "def MiCRM_dxx_numba_wrapper(t, x, p):\n",
    "    # Unpack parameters directly for the JIT function\n",
    "    return MiCRM_dxx_jit(x, t,\n",
    "                         p['N'], p['M'],\n",
    "                         p['u'], p['l'],\n",
    "                         p['rho'], p['omega'],\n",
    "                         p['m'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e8b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eff_LV_params(p, sol, verbose=False):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of effective Lotka–Volterra parameters.\n",
    "    \"\"\"\n",
    "    M, N = p['M'], p['N']\n",
    "    l    = p['l']        # (N, M, M)\n",
    "    rho  = p['rho']      # (M,)\n",
    "    omega= p['omega']    # (M,)\n",
    "    m    = p['m']        # (N,)\n",
    "    u    = p['u']        # (N, M)\n",
    "    lam  = p['lambda']   # (N, M)\n",
    "\n",
    "    # Equilibrium values\n",
    "    Ceq = sol.y[:N, -1]  # (N,)\n",
    "    Req = sol.y[N:, -1]  # (M,)\n",
    "\n",
    "    # 1) Build A (M×M):\n",
    "    #    A = -diag(omega) + ∑_i [ l[i,α,β]*u[i,β]*Ceq[i] ]  -  diag_i( ∑_i u[i,α]*Ceq[i] )\n",
    "    #\n",
    "    # First term: -omega on diagonal\n",
    "    A = -np.diag(omega)\n",
    "\n",
    "    # add sum_i l[i,:,:] * (u[i,:]*Ceq[i]) broadcasted over β\n",
    "    # compute W[i,β] = u[i,β]*Ceq[i]\n",
    "    W = u * Ceq[:, None]           # (N, M)\n",
    "    # Now add ∑_i l[i,α,β] * W[i,β]  → sum over i,β\n",
    "    # We want for each (α,β): sum_i l[i,α,β]*W[i,β]\n",
    "    # That is: contract i and β\n",
    "    A += np.tensordot(W, l, axes=([0,1],[0,2]))  # yields (M, M)\n",
    "\n",
    "    # subtract ∑_i u[i,β]*Ceq[i] on the diagonal\n",
    "    diag_sub = W.sum(axis=0)       # (M,)\n",
    "    A[np.diag_indices(M)] -= diag_sub\n",
    "\n",
    "    # 2) Inverse of A\n",
    "    invA = np.linalg.inv(A)\n",
    "\n",
    "    # 3) Compute dR = ∂R/∂C  (M×N):\n",
    "    #    dR[α,j] = ∑_{β,γ} invA[α,β]*u[j,β]*Req[γ]*(δ(β,γ) - l[j,β,γ])\n",
    "    #\n",
    "    # Define T[j,β,γ] = u[j,β]*Req[γ]*(δ(β,γ) - l[j,β,γ])\n",
    "    #   δ(β,γ) - l[j,β,γ] → eye(M) - l[j]\n",
    "    eyeM = np.eye(M)\n",
    "    # broadcast to (N, M, M):\n",
    "    D = (eyeM[None,:,:] - l)       # (N, M, M)\n",
    "    T = u[:,:,None] * Req[None,None,:] * D  # (N, M, M)\n",
    "\n",
    "    # Now dR[α,j] = ∑_{β,γ} invA[α,β] * T[j,β,γ]\n",
    "    #   = for each j: row α of invA dot T[j,β,:].sum over γ\n",
    "    # First sum T over γ: S[j,β] = ∑_γ T[j,β,γ]\n",
    "    S = T.sum(axis=2)              # (N, M)\n",
    "    # Now for each j: dR[:,j] = invA @ S[j,:]\n",
    "    dR = (invA @ S.T)              # (M, N)\n",
    "\n",
    "    # 4) Effective interaction matrix α = A_thing @ dR\n",
    "    A_thing = u * (1 - lam)        # (N, M)\n",
    "    # α[i,j] = ∑_α A_thing[i,α] * dR[α,j]\n",
    "    alpha = A_thing @ dR           # (N, N)\n",
    "\n",
    "    # 5) Intrinsic growth r = O - P - m\n",
    "    #    O[i] = ∑_α A_thing[i,α] * Req[α]\n",
    "    #    P[i] = ∑_j α[i,j] * Ceq[j]\n",
    "    O = A_thing @ Req              # (N,)\n",
    "    P = alpha @ Ceq                # (N,)\n",
    "    r_eff = O - P - m              # (N,)\n",
    "\n",
    "    result = {'alpha': alpha, 'r': r_eff, 'N': N}\n",
    "    if verbose:\n",
    "        result.update({'dR': dR, 'A': A})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a0e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LV_dx(x, t, p):\n",
    "    \"\"\"\n",
    "    Vectorized Lotka–Volterra RHS.\n",
    "    \n",
    "    x : (N,) array of species abundances\n",
    "    p : dict with keys:\n",
    "        - 'r'     : (N,) intrinsic growth rates\n",
    "        - 'alpha' : (N, N) interaction matrix\n",
    "        - 'N'     : number of species (optional, not used here)\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    r     = p['r']        # (N,)\n",
    "    alpha = p['alpha']    # (N, N)\n",
    "    \n",
    "    # compute interaction term: alpha @ x  → (N,)\n",
    "    interaction = alpha.dot(x)\n",
    "    \n",
    "    # dx_i = x_i * (r_i + interaction_i)\n",
    "    dx = x * (r + interaction)\n",
    "    \n",
    "    # zero out extremely small populations if you want the same “spacing” check\n",
    "    # e.g. dx[x <= np.spacing(x)] = 0.0\n",
    "    \n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c58ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from github Jacobian.jl, converted to python\n",
    "\n",
    "# this computes the Jacobian matrix \n",
    "# useful for downstream analysis on stability against perturbations \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eff_LV_jac(p_lv, sol, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Vectorized Jacobian of the effective GLV at equilibrium,\n",
    "    restricted to species with biomass > threshold.\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    alpha_full = p_lv['alpha']   # (N,N)\n",
    "    N_full     = p_lv['N']\n",
    "    bm         = sol.y[:N_full, -1]  # (N,)\n",
    "\n",
    "    # feasible indices\n",
    "    feasible = np.where(bm > threshold)[0]\n",
    "    if feasible.size == 0:\n",
    "        raise ValueError(\"No feasible species found!\")\n",
    "    \n",
    "    # subset\n",
    "    C    = bm[feasible]                                # (n,)\n",
    "    alpha = alpha_full[np.ix_(feasible, feasible)]     # (n,n)\n",
    "\n",
    "    # Jacobian J_ij = alpha_ij * C_i\n",
    "    # can be written as diag(C) @ alpha\n",
    "    J = np.diag(C) @ alpha                             # (n,n)\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def MiCRM_jac(p, sol):\n",
    "    \"\"\"\n",
    "    Vectorized full Jacobian of the MiCRM system at equilibrium.\n",
    "    \"\"\"\n",
    "    N, M = p['N'], p['M']\n",
    "    lam   = p['lambda']   # (N,M)\n",
    "    l     = p['l']        # (N,M,M)\n",
    "    omega = p['omega']    # (M,)\n",
    "    m     = p['m']        # (N,)\n",
    "    u     = p['u']        # (N,M)\n",
    "\n",
    "    # Equilibrium state\n",
    "    state = sol.y[:, -1]\n",
    "    C     = state[:N]     # (N,)\n",
    "    R     = state[N:]     # (M,)\n",
    "\n",
    "    # --- Consumer–Consumer block (N×N) ---\n",
    "    cc_diag = -m + ((1 - lam) * u * R[None, :]).sum(axis=1)\n",
    "    CC = np.diag(cc_diag)\n",
    "\n",
    "    # --- Consumer–Resource block (N×M) ---\n",
    "    CR = C[:, None] * (1 - lam) * u  # (N,M)\n",
    "\n",
    "    # --- Resource–Resource block (M×M) ---\n",
    "    # P[i,α,β] = C[i]*u[i,α]*l[i,α,β]\n",
    "    P = C[:, None, None] * u[:, :, None] * l  # (N,M,M)\n",
    "    RR = P.sum(axis=0)                        # (M,M)\n",
    "\n",
    "    # Correct diagonal:\n",
    "    # diag_val[α] = ∑_i C[i]*u[i,α]*l[i,α,α]  = RR[α,α]\n",
    "    diag_val = np.diag(RR)                    # (M,)\n",
    "    # sub_diag[α] = ∑_i C[i]*u[i,α]\n",
    "    sub_diag = (C[:, None] * u).sum(axis=0)   # (M,)\n",
    "    # new diagonal = diag_val - sub_diag - omega\n",
    "    diag_rr = diag_val - sub_diag - omega     # (M,)\n",
    "    np.fill_diagonal(RR, diag_rr)\n",
    "\n",
    "    # --- Resource–Consumer block (M×N) ---\n",
    "    # term1 = -u * R  ; term2 = ∑_β u[i,β]*R[β]*l[i,β,α]\n",
    "    Q = u * R[None, :]                        # (N,M)\n",
    "    Ql = Q[:, :, None] * l                    # (N,M,M)\n",
    "    term2 = Ql.sum(axis=1)                    # (N,M)\n",
    "    RC = (term2 - Q).T                        # (M,N)\n",
    "\n",
    "    # Assemble\n",
    "    top    = np.hstack([CC, CR])              # (N, N+M)\n",
    "    bottom = np.hstack([RC, RR])              # (M, N+M)\n",
    "    return np.vstack([top, bottom])           # (N+M, N+M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### BELOW CODE SHOULD BE APPLICABLE FOR BOTH GLVM + MICRM EIGENVALUE CALCULATIONS #####\n",
    "\n",
    "def leading_eigenvalue(J): \n",
    "    \"\"\"\n",
    "    Compute the dominant eigenvalue (largest real part) of Jacobian matrix J.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(J)\n",
    "    return eigvals[np.argmax(np.real(eigvals))]\n",
    "\n",
    "\n",
    "def hermitian_part(J):\n",
    "    \"\"\"\n",
    "    Return the Hermitian (symmetric) part of a real matrix J: (J + J.T) / 2.\n",
    "    \"\"\"\n",
    "    return (J + J.T) / 2\n",
    "\n",
    "\n",
    "def leading_hermitian_eigenvalue(J):\n",
    "    \"\"\"\n",
    "    Compute the leading eigenvalue of the Hermitian part of J,\n",
    "    which indicates the reactivity of the system.\n",
    "    \"\"\"\n",
    "    H = hermitian_part(J)\n",
    "    # For symmetric H, use eigvalsh for efficiency and guaranteed real outputs\n",
    "    eigvals = np.linalg.eigvalsh(H)\n",
    "    return np.max(eigvals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e60917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nremoved some irrelevant functions previously in code\\n\\n- cosine similarity\\n- euclidean distance\\n- bray curtis dissimilarity (have modified and included this later on) \\n- AIC - Akaike information criterion \\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from github sim_frame.jl, converted to python. sim_frame.jl includes the previous 6 files.\n",
    " \n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.stats import dirichlet\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "# Custom MiCRM‐parameter‐generator functions (override defaults)\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "def F_m(N, M, kw):\n",
    "    \"\"\"\n",
    "    Temperature‐dependent mortality (maintenance) rates.\n",
    "    If 'tt' is in kw (an Nx2 array of TPC scalings), use the 2nd column;\n",
    "    otherwise fallback to a constant 0.2 for each species.\n",
    "    \"\"\"\n",
    "    if 'tt' in kw:\n",
    "        # kw['tt'] assumed shape (N,2)\n",
    "        return kw['tt'][:, 1] # this is the respiration/mortality part of 'tt', defined before. used to be temp_p_m.\n",
    "    else:\n",
    "        return np.full(N, 0.2)\n",
    "\n",
    "def F_rho(N, M, kw):\n",
    "    \"\"\"\n",
    "    Resource supply rates: constant ones by default.\n",
    "    \"\"\"\n",
    "    return np.ones(M)\n",
    "\n",
    "# could introduce temperature-dependence in this if we want to look at temp-dependent resource supply \n",
    "# same with omega below, seeing how temp affects loss/dilution of resources in specific ecosystems \n",
    "\n",
    "def F_omega(N, M, kw):\n",
    "    \"\"\"\n",
    "    Resource loss/dilution rates: zero by default.\n",
    "    \"\"\"\n",
    "    return np.zeros(M)\n",
    "\n",
    "def F_u(N, M, kw):\n",
    "    \"\"\"\n",
    "    Temperature‐scaled uptake matrix.\n",
    "      - Draws N Dirichlet(1,…,1) rows of length M (relative preference).\n",
    "      - Scales each row i by kw['tt'][i,0] if present, else by 2.5.\n",
    "    \"\"\"\n",
    "    # draw relative preferences\n",
    "    diri = np.stack([rng.dirichlet(np.ones(M)) for _ in range(N)], axis=0)\n",
    "    \n",
    "    if 'tt' in kw:\n",
    "        u_sum = kw['tt'][:, 0] # this is the uptake part of 'tt', defined before. used to be temp_p_u. \n",
    "    else:\n",
    "        u_sum = np.full(N, 2.5)\n",
    "    \n",
    "    # scale each row by its TPC magnitude\n",
    "    return diri * u_sum[:, None]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "removed some irrelevant functions previously in code\n",
    "\n",
    "- cosine similarity\n",
    "- euclidean distance\n",
    "- bray curtis dissimilarity (have modified and included this later on) \n",
    "- AIC - Akaike information criterion \n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de27e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for diversity \n",
    "\n",
    "\n",
    "##### Shannon diversity #####\n",
    "\n",
    "def shannon (abundance): # abundance should be an array (incl values for all consumers in system) \n",
    "    C_shannon = np.asarray(abundance, dtype=float) # convert into numpy arrays of floats \n",
    "  \n",
    "    ### normalise to convert abundance into something on a scale of 0-1 ###\n",
    "\n",
    "    # total number of 'individuals' (add up all the relative abundances) \n",
    "    total_abundance = np.sum(C_shannon) # add up all the elements in this C_shannon array. total_abundance also a single value. \n",
    "\n",
    "    pi = C_shannon / total_abundance # pi is  an array. it now converts C_shannon into relative proportions, by dividing each element of C_shannon by the total_abundance value. \n",
    "\n",
    "    pi_lnpi = pi[pi > 0] * np.log(pi[pi > 0]) # this is pi * ln(pi). pi_lnpi should also be an array with N elements. \n",
    "    # keep only the pi > 0 ones. if pi = 0, will have issues with log. if pi = 0 it won't contribute to Shannon index anyway. \n",
    "\n",
    "    H = -np.sum(pi_lnpi) \n",
    "\n",
    "    return H \n",
    "\n",
    "# note: for Shannon diversity, might need to consider the case where total abundance is 0 (so would be dividing by 0)\n",
    "\n",
    "# can compare Shannon diversity for 2 samples (GLV vs MiCRM for each temperature)\n",
    "\n",
    "##### Bray-Curtis dissimilarity ##### \n",
    "\n",
    "# defined earlier already, but re-write anyway \n",
    "\n",
    "\n",
    "def bray_curtis_dissimilarity(G, M): # G = GLV, M = MiCRM \n",
    "\n",
    "    G_array = np.asarray(G, dtype=float) # convert G (abundance of each species predicted by GLV) into array \n",
    "    M_array = np.asarray(M, dtype=float) # convert M (abundance of each species predicted by MiCRM) into array \n",
    "\n",
    "    G_safe = np.where(G_array < 0, 0, G_array) # if any element (species) of the GLV-predicted array of abundances is less than 0, consider that 0\n",
    "    M_safe = np.where(M_array < 0, 0, M_array) \n",
    "\n",
    "    GM_dissimilarity = np.sum(np.abs(G_safe - M_safe)) / np.sum(G_safe + M_safe) # bray-curtis dissimilarity between GLV and MiCRM predictions \n",
    "\n",
    "    return GM_dissimilarity \n",
    "\n",
    "# also consider the case where total abundance is 0 (so would be dividing by 0) \n",
    "\n",
    "\n",
    "### Jaccard ###\n",
    "\n",
    "def jaccard_index(G, M, thresh=1e-8):\n",
    "    \"\"\"\n",
    "    Jaccard index J = |A ∩ B| / |A ∪ B| for presence/absence sets.\n",
    "    Presence := abundance > thresh.\n",
    "    Returns 1.0 if both sets are empty.\n",
    "    \"\"\"\n",
    "    G = np.asarray(G, dtype=float)\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    if G.shape != M.shape:\n",
    "        raise ValueError(\"G and M must have same shape\")\n",
    "    G_surv = G > thresh\n",
    "    M_surv = M > thresh\n",
    "    inter = np.logical_and(G_surv, M_surv).sum()\n",
    "    union = np.logical_or(G_surv, M_surv).sum()\n",
    "    return 1.0 if union == 0 else inter / union\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2047a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define functions for abundance deviation + species overlap between MiCRM and EGLV ###\n",
    "\n",
    "\"\"\"\n",
    "this is for each community simulation (N consumers), for each temperature\n",
    "to be incorporated into the later functions and loops (loop this function for each temp, then for each community simulation)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def err_eq_and_overlap(C_LV_eq, C_MiCRM_eq, thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "    - Mean log-ratio error in predicted abundances (GLV vs MiCRM) \n",
    "    - Number of overlapping survivors (identity-based overlap)\n",
    "\n",
    "    Only includes species that are predicted to survive by *both* models.\n",
    "\n",
    "    Inputs:\n",
    "    - C_LV_eq: GLV-predicted equilibrium consumer abundances (array-like)\n",
    "    - C_MiCRM_eq: MiCRM-predicted equilibrium consumer abundances (array-like)\n",
    "    - thresh: threshold below which species are considered extinct\n",
    "    - eps: small pseudocount to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    - equilibrium_error: mean log-ratio error between GLV and MiCRM (NaN if no shared survivors)\n",
    "    - overlap_count: number of species predicted to survive by both models\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to float arrays\n",
    "    C_LV = np.asarray(C_LV_eq, dtype=float)\n",
    "    C_Mi = np.asarray(C_MiCRM_eq, dtype=float)\n",
    "\n",
    "    # Identify survivors (boolean masks)\n",
    "    G_surv = C_LV > thresh\n",
    "    M_surv = C_Mi > thresh\n",
    "\n",
    "    # Overlap mask: species that survive in BOTH models\n",
    "    overlap_mask = G_surv & M_surv # element-wise check, so overlap_mask is true only if BOTH G_surv and M_surv are true for a species \n",
    "    overlap_count = np.sum(overlap_mask) # counts how many TRUE entries there are (size of intersection for survivors) \n",
    "\n",
    "    if overlap_count == 0:\n",
    "        return np.nan, 0  # No shared survivors, can't compute meaningful abundance deviation\n",
    "\n",
    "\n",
    "    # Log ratio of overlapping species\n",
    "    log_ratios = np.log(C_LV[overlap_mask] / C_Mi[overlap_mask])\n",
    "    equilibrium_error = np.mean(log_ratios)\n",
    "\n",
    "    return equilibrium_error, overlap_count\n",
    "\n",
    "\n",
    "# define function to produce the Err(t) function showing deviations over time\n",
    "\n",
    "\n",
    "def err_time_series(times, C_LV_traj, C_Mi_traj, thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Vectorized Err(t) and overlap count arrays.\n",
    "    \"\"\"\n",
    "    # Boolean mask of shape (N, T)\n",
    "    mask = (C_LV_traj > thresh) & (C_Mi_traj > thresh)\n",
    "\n",
    "    # Count survivors at each timepoint: shape (T,)\n",
    "    overlap_counts = mask.sum(axis=0)\n",
    "\n",
    "    # Compute all log‐ratios, then null out non‐overlap entries\n",
    "    log_ratios = np.log(C_LV_traj / C_Mi_traj)\n",
    "    log_ratios[~mask] = np.nan\n",
    "\n",
    "    # Mean across species yields Err(t): shape (T,)\n",
    "    err_t = np.nanmean(log_ratios, axis=0)\n",
    "\n",
    "    return err_t, overlap_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define function to produce the Errtraj, which integrates Err(t) and finds area under curve. represents trajectory deviation. \n",
    "\n",
    "def integrate_err(\n",
    "    times,  # 1D array of time‐points, shape (T,)\n",
    "    err_t   # 1D array of instantaneous errors, shape (T,)\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute the time‐averaged trajectory error\n",
    "      Err_traj = (1/(t_end - t_start)) * ∫ Err(t) dt\n",
    "    using the trapezoid rule on the discrete grid.\n",
    "    Returns a single float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask out any NaNs (where overlap = 0)\n",
    "    valid     = ~np.isnan(err_t)\n",
    "    t_valid   = times[valid]   # times where err_t is real\n",
    "    err_valid = err_t[valid]   # corresponding error values\n",
    "\n",
    "    # Need at least 2 points to do any integration\n",
    "    if valid.sum() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    # Numerically integrate Err(t) dt by the trapezoid rule:\n",
    "    integral = np.trapz(err_valid, x=t_valid)\n",
    "\n",
    "    # Divide by total time to get *average* error\n",
    "    duration = t_valid[-1] - t_valid[0]  # here, t_valid[-1] == t_end\n",
    "    return integral / duration\n",
    "\n",
    "\n",
    "# when finding trajectory error, we only want to find it up to teq (time where both systems reach equilibrium) \n",
    "# hence we define this function below to work out the equilibrium states \n",
    "\n",
    "\n",
    "def estimate_teq(times, sol, sol_lv, tol=1e-6, window=5):\n",
    "    \"\"\"\n",
    "    Find the first index j where both sol.y and sol_lv.y\n",
    "    stop changing by more than tol for `window` consecutive steps.\n",
    "    \"\"\"\n",
    "    # Truncate to shared time length\n",
    "    T = min(sol.y.shape[1], sol_lv.y.shape[1])\n",
    "    Y1 = sol.y[:sol_lv.y.shape[0], :T]  # (N, T)\n",
    "    Y2 = sol_lv.y[:, :T]                # (N, T)\n",
    "\n",
    "    # Compute max change between successive steps\n",
    "    d1 = np.max(np.abs(np.diff(Y1, axis=1)), axis=0)\n",
    "    d2 = np.max(np.abs(np.diff(Y2, axis=1)), axis=0)\n",
    "\n",
    "    combined = np.maximum(d1, d2)\n",
    "    combined = np.concatenate([[np.inf], combined])  # pad to same size as T\n",
    "\n",
    "    for j in range(T - window + 1):\n",
    "        if np.all(combined[j : j + window] < tol):\n",
    "            return j\n",
    "\n",
    "    return T - 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b4d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Investigating the effect of temperature on higher-order interactions\n",
    "Hessian = second derivative \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "# 1) A Numba-friendly version of F_of_C_vec that takes each p-field as an argument.\n",
    "@njit\n",
    "def F_of_C_jit(C, N, M, u, lam, rho, omega, l, m):\n",
    "    \"\"\"\n",
    "    dC/dt with resources at equilibrium, Numaby-compiled.\n",
    "    C       : (N,)\n",
    "    u, lam  : (N, M)\n",
    "    rho,omega: (M,)\n",
    "    l       : (N, M, M)\n",
    "    m       : (N,)\n",
    "    \"\"\"\n",
    "    # Build A matrix\n",
    "    A = -np.diag(omega)\n",
    "    for i in range(N):\n",
    "        for b in range(M):\n",
    "            Wib = u[i, b] * C[i]\n",
    "            # add ∑_i l[i,b,α] * Wib\n",
    "            for a in range(M):\n",
    "                A[a, b] += l[i, a, b] * Wib\n",
    "    # subtract diagonal ∑_i u[i,α]*C[i]\n",
    "    for a in range(M):\n",
    "        s = 0.0\n",
    "        for i in range(N):\n",
    "            s += u[i, a] * C[i]\n",
    "        A[a, a] -= s\n",
    "\n",
    "    # solve A @ R_star = rho\n",
    "    R_star = np.linalg.solve(A, rho)\n",
    "\n",
    "    # compute net growth\n",
    "    net = np.empty(N)\n",
    "    for i in range(N):\n",
    "        s = 0.0\n",
    "        for a in range(M):\n",
    "            s += (1 - lam[i, a]) * u[i, a] * R_star[a]\n",
    "        net[i] = s - m[i]\n",
    "\n",
    "    # return dC/dt\n",
    "    dC = np.empty(N)\n",
    "    for i in range(N):\n",
    "        dC[i] = C[i] * net[i]\n",
    "    return dC\n",
    "\n",
    "# 2) Numba-accelerated Hessian norm, calling the jitted F_of_C_jit\n",
    "@njit(parallel=True)\n",
    "def compute_hessian_norm_nb(C_eq, N, M, u, lam, rho, omega, l, m, eps=1e-6):\n",
    "    H2_sum = 0.0\n",
    "    for j in prange(N):\n",
    "        for k in range(N):\n",
    "            # build perturbation vectors\n",
    "            C_pp = C_eq.copy(); C_pp[j] += eps; C_pp[k] += eps\n",
    "            C_pm = C_eq.copy(); C_pm[j] += eps; C_pm[k] -= eps\n",
    "            C_mp = C_eq.copy(); C_mp[j] -= eps; C_mp[k] += eps\n",
    "            C_mm = C_eq.copy(); C_mm[j] -= eps; C_mm[k] -= eps\n",
    "\n",
    "            # central differences\n",
    "            F_pp = F_of_C_jit(C_pp, N, M, u, lam, rho, omega, l, m)\n",
    "            F_pm = F_of_C_jit(C_pm, N, M, u, lam, rho, omega, l, m)\n",
    "            F_mp = F_of_C_jit(C_mp, N, M, u, lam, rho, omega, l, m)\n",
    "            F_mm = F_of_C_jit(C_mm, N, M, u, lam, rho, omega, l, m)\n",
    "\n",
    "            for i in range(N):\n",
    "                d2 = (F_pp[i] - F_pm[i] - F_mp[i] + F_mm[i])\n",
    "                H2_sum += (d2 * d2) / (4 * eps * eps)\n",
    "\n",
    "    return np.sqrt(H2_sum)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1821bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_row_diagonally_dominant(J, tol=1.0):\n",
    "    \"\"\"\n",
    "    Check if every row of square matrix J is diagonally dominated:\n",
    "        |J[ii]| >= tol * sum_{j!=i} |J[ij]|\n",
    "    Returns a boolean array of length J.shape[0], True where row i is dominated.\n",
    "    \"\"\"\n",
    "    J = np.asarray(J)\n",
    "    absJ = np.abs(J)\n",
    "    diag = absJ.diagonal()\n",
    "    offdiag_sums = absJ.sum(axis=1) - diag\n",
    "    return diag >= tol * offdiag_sums\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If the rows are no longer diagonally dominant (e.g. at higher temperatures),\n",
    "then it's not valid to use the shortcut to estimate timescale separation. \n",
    "Would need to use the function below. \n",
    "\n",
    "Option 1) just use the full method \n",
    "Option 2) use the shortcut, but justify by showing that it's still valid (diagonally dominant) \n",
    "\n",
    "Learning toward using option 1) because we could argue that higher temp -> less diagonal dominance (more interactions) -> more reactivity \n",
    "\"\"\"\n",
    "\n",
    "def timescale_separation_full(J_full, N):\n",
    "    \"\"\"\n",
    "    Given the full (N+M)x(N+M) MiCRM Jacobian J_full and number of consumers N,\n",
    "    returns (tau_C, tau_R, epsilon) by eigenmode classification.\n",
    "    \"\"\"\n",
    "    # 1) eigendecompose\n",
    "    eigvals, eigvecs = np.linalg.eig(J_full)\n",
    "    re_times = 1.0 / np.abs(np.real(eigvals))\n",
    "\n",
    "    # 2) classify modes by where v has more weight\n",
    "    #    sum |v[i]| over consumers vs. resources\n",
    "    weights = np.abs(eigvecs)\n",
    "    cons_weight = weights[:N, :].sum(axis=0)\n",
    "    res_weight  = weights[N:, :].sum(axis=0)\n",
    "\n",
    "    cons_mask = cons_weight >= res_weight\n",
    "    res_mask  = ~cons_mask\n",
    "\n",
    "    # 3) select timescales\n",
    "    #    - consumers: fastest return (smallest tau)\n",
    "    #    - resources: slowest return (largest tau)\n",
    "    if not np.any(cons_mask) or not np.any(res_mask):\n",
    "        raise ValueError(\"No pure consumer or resource modes found!\")\n",
    "    tau_C = np.min(re_times[cons_mask])\n",
    "    tau_R = np.max(re_times[res_mask])\n",
    "\n",
    "    # 4) separation\n",
    "    epsilon = tau_C / tau_R\n",
    "    return tau_C, tau_R, epsilon\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5129cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def full_glv_jacobian(p_lv, C_lv_eq):\n",
    "    \"\"\"Build the full N×N GLV consumer‐block Jacobian at C_lv_eq.\"\"\"\n",
    "    alpha = p_lv['alpha']\n",
    "    r     = p_lv['r']\n",
    "    # diag(r + α·C)\n",
    "    diag_term = r + alpha.dot(C_lv_eq)\n",
    "    Jdiag = np.diag(diag_term)\n",
    "    # off‐diag: C_i * α_{ij}\n",
    "    Joff  = np.diag(C_lv_eq) @ alpha\n",
    "    return Jdiag + Joff\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "above, we need to define the full glv jacobian for the sake of this analysis \n",
    "because we need a 1:1 comparison between the micrm jacobian and glv jacobian \n",
    "the eff_LV_jac that we defined before is a reduced version that only keeps feasible species (e.g. for analysis of stability, reactivity etc)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def bauer_fike_bound(J_micrm_full, p_lv, C_mi_eq, C_lv_eq, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Bauer–Fike bound for |Δλ| using only survivors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    J_micrm_full : (N+M)x(N+M) array\n",
    "        Full MiCRM Jacobian.\n",
    "    p_lv : dict\n",
    "        GLV params, must contain 'alpha', 'r', 'N'.\n",
    "    C_mi_eq : (N,) array\n",
    "        Equilibrium MiCRM consumer abundances.\n",
    "    C_lv_eq : (N,) array\n",
    "        Equilibrium GLV consumer abundances.\n",
    "    threshold : float\n",
    "        Minimum abundance to call a species 'surviving'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        BF bound.\n",
    "    \"\"\"\n",
    "    # 1) survivors in MiCRM\n",
    "    feas = np.where(C_mi_eq > threshold)[0]\n",
    "    if feas.size == 0:\n",
    "        raise ValueError(\"No feasible species found above threshold.\")\n",
    "\n",
    "    # 2) subset MiCRM-consumer block\n",
    "    N = C_mi_eq.size\n",
    "    Jr_full = J_micrm_full[:N, :N]\n",
    "    Jr = Jr_full[np.ix_(feas, feas)]\n",
    "\n",
    "    # 3) build full GLV consumer-block jacobian and subset\n",
    "    Jg_full = full_glv_jacobian(p_lv, C_lv_eq)\n",
    "    Jg = Jg_full[np.ix_(feas, feas)]\n",
    "\n",
    "    # 4) compute difference norm\n",
    "    ΔJ = Jr - Jg\n",
    "    normΔ = np.linalg.norm(ΔJ, ord='fro')\n",
    "\n",
    "    # 5) get eigenvector condition number\n",
    "    eigvals, V = np.linalg.eig(Jr)\n",
    "    condV = np.linalg.norm(V, 2) * np.linalg.norm(np.linalg.inv(V), 2)\n",
    "\n",
    "    return condV * normΔ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a497495",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25128\\1345020035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_rng\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m from pandas.core.api import (\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;31m# dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mvalue_counts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboolean\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooleanDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m from pandas.core.arrays.floating import (\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFloatingArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIntegerArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIntervalArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseMaskedArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPandasArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\interval.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m from pandas.core.ops import (\n\u001b[0;32m     92\u001b[0m     \u001b[0minvalid_comparison\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseMaskedArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m from pandas.core.base import (\n\u001b[0;32m    145\u001b[0m     \u001b[0mIndexOpsMixin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chan Li\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from numpy.random import default_rng\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# ─── Ensure output directory ────────────────────────\n",
    "outdir = \"output\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ─── Simulation parameters ─────────────────────────\n",
    "rng = default_rng(111)\n",
    "N, M = 70, 35            # adjust for full run\n",
    "L = np.full(N, 0.3)\n",
    "x0 = np.concatenate([np.full(N, 0.1), np.full(M, 1)])\n",
    "temp_vals = np.linspace(273.15, 273.15 + 30, 31)\n",
    "rho_t = np.array([0.0, 0.0])\n",
    "Tr = 273.15 + 10\n",
    "Ed = 3.5\n",
    "\n",
    "# ─── Per‐replicate function capturing raw, metrics, trajectory ─────\n",
    "def run_single_full_raw(replicate_id):\n",
    "    structural = generate_params(\n",
    "        N, M,\n",
    "        f_u=def_u, f_m=def_m, f_rho=def_rho,\n",
    "        f_omega=def_omega, f_l=def_l,\n",
    "        L=L, T=273.15, rho_t=rho_t, Tr=Tr, Ed=Ed\n",
    "    )\n",
    "    raw_rows = []\n",
    " \n",
    "    for T in temp_vals:\n",
    "        # build parameters\n",
    "        temp_p, _, _, _ = temp_trait(N, {'T': T, 'Tr': Tr, 'Ed': Ed, 'rho_t': rho_t, 'L': L})\n",
    "        pT = { **structural,\n",
    "              'u': structural['u'] * temp_p[:,0][:,None],\n",
    "              'm': temp_p[:,1],\n",
    "              'lambda': np.sum(structural['l'], axis=2),\n",
    "              'T': T }\n",
    "\n",
    "        # SOLVE EQUILIBRIUM\n",
    "        sol = solve_ivp(lambda t,y: MiCRM_dxx_numba_wrapper(t,y,pT),\n",
    "                        (0,300), x0, method='LSODA', rtol=1e-4, atol=1e-7)\n",
    "        p_lv = eff_LV_params(pT, sol, verbose=False)\n",
    "        sol_lv = solve_ivp(lambda t,y: LV_dx(y,t,p_lv),\n",
    "                           (0,300), sol.y[:N,0], method='LSODA', rtol=1e-4, atol=1e-7)\n",
    "\n",
    "        C_eq_mi = sol.y[:N,-1]\n",
    "        C_eq_lv = sol_lv.y[:N,-1]\n",
    "\n",
    "        # RAW equilibrium vectors\n",
    "        raw = {'replicate': replicate_id, 'T_K': T, 'T_C': T-273.15}\n",
    "        for i in range(N):\n",
    "            raw[f'Cmi_{i}'] = C_eq_mi[i]\n",
    "            raw[f'Clv_{i}'] = C_eq_lv[i]\n",
    "        raw_rows.append(raw)\n",
    "\n",
    "              \n",
    "\n",
    "    return raw_rows\n",
    "\n",
    "# ─── Main: 2-rep test, then full parallel run ───────\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    full_ids = list(range(1,101))\n",
    "    with mp.Pool(8) as pool:\n",
    "        results = pool.map(run_single_full_raw, full_ids)\n",
    "\n",
    "    raw_full = [row for r in results for row in r]\n",
    "\n",
    "    pd.DataFrame(raw_full).to_csv(os.path.join(outdir,'raw_eq.csv'), index=False)\n",
    "\n",
    "    print(\"Full parallel run done: raw_eq.csv\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
